{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boston_Housing.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1d6EMJc7adB_",
        "colab_type": "code",
        "outputId": "1c3f2855-2f7d-492f-8c4c-d6d5b858c959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow\n",
        "#!pip install keras \n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nyYiP7CYanzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import boston_housing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RBDzeo6mbHqj",
        "colab_type": "code",
        "outputId": "a5cd6579-7e92-4574-e321-4b93fbabded1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "juzigzGGb8Ko",
        "colab_type": "code",
        "outputId": "49f7615b-eeba-4e24-8f8d-56b6c6e87ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape, train_targets.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (404,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "I1Deu9tmce_C",
        "colab_type": "code",
        "outputId": "8c8b9b04-20d8-425e-8d6d-12a164cd944d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "type(train_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "Yvjn3zp2coGF",
        "colab_type": "code",
        "outputId": "b687c64e-2e70-40ae-fc2e-d137fa00ebaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.23247e+00, 0.00000e+00, 8.14000e+00, ..., 2.10000e+01,\n",
              "        3.96900e+02, 1.87200e+01],\n",
              "       [2.17700e-02, 8.25000e+01, 2.03000e+00, ..., 1.47000e+01,\n",
              "        3.95380e+02, 3.11000e+00],\n",
              "       [4.89822e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
              "        3.75520e+02, 3.26000e+00],\n",
              "       ...,\n",
              "       [3.46600e-02, 3.50000e+01, 6.06000e+00, ..., 1.69000e+01,\n",
              "        3.62250e+02, 7.83000e+00],\n",
              "       [2.14918e+00, 0.00000e+00, 1.95800e+01, ..., 1.47000e+01,\n",
              "        2.61950e+02, 1.57900e+01],\n",
              "       [1.43900e-02, 6.00000e+01, 2.93000e+00, ..., 1.56000e+01,\n",
              "        3.76700e+02, 4.38000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "XwF5h6esc-J2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ynh29H1cd6uS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LW_lX86eCxq",
        "colab_type": "code",
        "outputId": "797b88c7-5bee-4dc9-925d-1db36a6acab8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1969
        }
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.23247</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5380</td>\n",
              "      <td>6.142</td>\n",
              "      <td>91.7</td>\n",
              "      <td>3.9769</td>\n",
              "      <td>4.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>18.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02177</td>\n",
              "      <td>82.5</td>\n",
              "      <td>2.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4150</td>\n",
              "      <td>7.610</td>\n",
              "      <td>15.7</td>\n",
              "      <td>6.2700</td>\n",
              "      <td>2.0</td>\n",
              "      <td>348.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>395.38</td>\n",
              "      <td>3.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.89822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6310</td>\n",
              "      <td>4.970</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.3325</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>375.52</td>\n",
              "      <td>3.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5150</td>\n",
              "      <td>6.037</td>\n",
              "      <td>34.5</td>\n",
              "      <td>5.9853</td>\n",
              "      <td>5.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>8.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.69311</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7130</td>\n",
              "      <td>6.376</td>\n",
              "      <td>88.4</td>\n",
              "      <td>2.5671</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>391.43</td>\n",
              "      <td>14.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.28392</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4930</td>\n",
              "      <td>5.708</td>\n",
              "      <td>74.3</td>\n",
              "      <td>4.7211</td>\n",
              "      <td>5.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>19.6</td>\n",
              "      <td>391.13</td>\n",
              "      <td>11.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>9.18702</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7000</td>\n",
              "      <td>5.536</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.5804</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>23.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4.09740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8710</td>\n",
              "      <td>5.468</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.4118</td>\n",
              "      <td>5.0</td>\n",
              "      <td>403.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>26.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2.15505</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8710</td>\n",
              "      <td>5.628</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.5166</td>\n",
              "      <td>5.0</td>\n",
              "      <td>403.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>169.27</td>\n",
              "      <td>16.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.62864</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6240</td>\n",
              "      <td>5.019</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.4394</td>\n",
              "      <td>4.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>21.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>34.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>9.59571</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6930</td>\n",
              "      <td>6.404</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.6390</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>376.11</td>\n",
              "      <td>20.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>18.81100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5970</td>\n",
              "      <td>4.628</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.5539</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>28.79</td>\n",
              "      <td>34.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.13914</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.05</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>5.572</td>\n",
              "      <td>88.5</td>\n",
              "      <td>2.5961</td>\n",
              "      <td>5.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>16.6</td>\n",
              "      <td>396.90</td>\n",
              "      <td>14.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>3.83684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7700</td>\n",
              "      <td>6.251</td>\n",
              "      <td>91.1</td>\n",
              "      <td>2.2955</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>350.65</td>\n",
              "      <td>14.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.38735</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5810</td>\n",
              "      <td>5.613</td>\n",
              "      <td>95.6</td>\n",
              "      <td>1.7572</td>\n",
              "      <td>2.0</td>\n",
              "      <td>188.0</td>\n",
              "      <td>19.1</td>\n",
              "      <td>359.29</td>\n",
              "      <td>27.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>73.53410</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6790</td>\n",
              "      <td>5.957</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.8026</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>16.45</td>\n",
              "      <td>20.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>6.53876</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.6310</td>\n",
              "      <td>7.016</td>\n",
              "      <td>97.5</td>\n",
              "      <td>1.2024</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>392.05</td>\n",
              "      <td>2.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.06466</td>\n",
              "      <td>70.0</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>6.345</td>\n",
              "      <td>20.1</td>\n",
              "      <td>7.8278</td>\n",
              "      <td>5.0</td>\n",
              "      <td>358.0</td>\n",
              "      <td>14.8</td>\n",
              "      <td>368.24</td>\n",
              "      <td>4.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.03445</td>\n",
              "      <td>82.5</td>\n",
              "      <td>2.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4150</td>\n",
              "      <td>6.162</td>\n",
              "      <td>38.4</td>\n",
              "      <td>6.2700</td>\n",
              "      <td>2.0</td>\n",
              "      <td>348.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>393.77</td>\n",
              "      <td>7.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.14866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5200</td>\n",
              "      <td>6.727</td>\n",
              "      <td>79.9</td>\n",
              "      <td>2.7778</td>\n",
              "      <td>5.0</td>\n",
              "      <td>384.0</td>\n",
              "      <td>20.9</td>\n",
              "      <td>394.76</td>\n",
              "      <td>9.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>37.66190</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6790</td>\n",
              "      <td>6.202</td>\n",
              "      <td>78.7</td>\n",
              "      <td>1.8629</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>18.82</td>\n",
              "      <td>14.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.02763</td>\n",
              "      <td>75.0</td>\n",
              "      <td>2.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4280</td>\n",
              "      <td>6.595</td>\n",
              "      <td>21.8</td>\n",
              "      <td>5.4011</td>\n",
              "      <td>3.0</td>\n",
              "      <td>252.0</td>\n",
              "      <td>18.3</td>\n",
              "      <td>395.63</td>\n",
              "      <td>4.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.01778</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4030</td>\n",
              "      <td>7.135</td>\n",
              "      <td>13.9</td>\n",
              "      <td>7.6534</td>\n",
              "      <td>3.0</td>\n",
              "      <td>402.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>384.30</td>\n",
              "      <td>4.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5380</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.03041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5150</td>\n",
              "      <td>5.895</td>\n",
              "      <td>59.6</td>\n",
              "      <td>5.6150</td>\n",
              "      <td>5.0</td>\n",
              "      <td>224.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>394.81</td>\n",
              "      <td>10.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9.82349</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6710</td>\n",
              "      <td>6.794</td>\n",
              "      <td>98.8</td>\n",
              "      <td>1.3580</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>21.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5240</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4690</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.67191</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5380</td>\n",
              "      <td>5.813</td>\n",
              "      <td>90.3</td>\n",
              "      <td>4.6820</td>\n",
              "      <td>4.0</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>376.88</td>\n",
              "      <td>14.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.17783</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.69</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5850</td>\n",
              "      <td>5.569</td>\n",
              "      <td>73.5</td>\n",
              "      <td>2.3999</td>\n",
              "      <td>6.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>395.77</td>\n",
              "      <td>15.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>0.07013</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5500</td>\n",
              "      <td>6.642</td>\n",
              "      <td>85.1</td>\n",
              "      <td>3.4211</td>\n",
              "      <td>5.0</td>\n",
              "      <td>276.0</td>\n",
              "      <td>16.4</td>\n",
              "      <td>392.78</td>\n",
              "      <td>9.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>0.05780</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.46</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4880</td>\n",
              "      <td>6.980</td>\n",
              "      <td>58.4</td>\n",
              "      <td>2.8290</td>\n",
              "      <td>3.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>0.04684</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.41</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4890</td>\n",
              "      <td>6.417</td>\n",
              "      <td>66.1</td>\n",
              "      <td>3.0923</td>\n",
              "      <td>2.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.18</td>\n",
              "      <td>8.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>4.81213</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7130</td>\n",
              "      <td>6.701</td>\n",
              "      <td>90.0</td>\n",
              "      <td>2.5975</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>255.23</td>\n",
              "      <td>16.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>0.05360</td>\n",
              "      <td>21.0</td>\n",
              "      <td>5.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4390</td>\n",
              "      <td>6.511</td>\n",
              "      <td>21.1</td>\n",
              "      <td>6.8147</td>\n",
              "      <td>4.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>16.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>0.12579</td>\n",
              "      <td>45.0</td>\n",
              "      <td>3.44</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4370</td>\n",
              "      <td>6.556</td>\n",
              "      <td>29.1</td>\n",
              "      <td>4.5667</td>\n",
              "      <td>5.0</td>\n",
              "      <td>398.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>382.84</td>\n",
              "      <td>4.56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>0.79041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.90</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5440</td>\n",
              "      <td>6.122</td>\n",
              "      <td>52.8</td>\n",
              "      <td>2.6403</td>\n",
              "      <td>4.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>18.4</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>0.32264</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6240</td>\n",
              "      <td>5.942</td>\n",
              "      <td>93.5</td>\n",
              "      <td>1.9669</td>\n",
              "      <td>4.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>21.2</td>\n",
              "      <td>378.25</td>\n",
              "      <td>16.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>0.55778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6240</td>\n",
              "      <td>6.335</td>\n",
              "      <td>98.2</td>\n",
              "      <td>2.1107</td>\n",
              "      <td>4.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>21.2</td>\n",
              "      <td>394.67</td>\n",
              "      <td>16.96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>0.04666</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4040</td>\n",
              "      <td>7.107</td>\n",
              "      <td>36.6</td>\n",
              "      <td>7.3090</td>\n",
              "      <td>2.0</td>\n",
              "      <td>329.0</td>\n",
              "      <td>12.6</td>\n",
              "      <td>354.31</td>\n",
              "      <td>8.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>0.03113</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4420</td>\n",
              "      <td>6.014</td>\n",
              "      <td>48.5</td>\n",
              "      <td>8.0136</td>\n",
              "      <td>3.0</td>\n",
              "      <td>352.0</td>\n",
              "      <td>18.8</td>\n",
              "      <td>385.64</td>\n",
              "      <td>10.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>0.17505</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4990</td>\n",
              "      <td>5.966</td>\n",
              "      <td>30.2</td>\n",
              "      <td>3.8473</td>\n",
              "      <td>5.0</td>\n",
              "      <td>279.0</td>\n",
              "      <td>19.2</td>\n",
              "      <td>393.43</td>\n",
              "      <td>10.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>9.92485</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7400</td>\n",
              "      <td>6.251</td>\n",
              "      <td>96.6</td>\n",
              "      <td>2.1980</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>388.52</td>\n",
              "      <td>16.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>0.11432</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5200</td>\n",
              "      <td>6.781</td>\n",
              "      <td>71.3</td>\n",
              "      <td>2.8561</td>\n",
              "      <td>5.0</td>\n",
              "      <td>384.0</td>\n",
              "      <td>20.9</td>\n",
              "      <td>395.58</td>\n",
              "      <td>7.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.41</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4890</td>\n",
              "      <td>7.079</td>\n",
              "      <td>63.1</td>\n",
              "      <td>3.4145</td>\n",
              "      <td>2.0</td>\n",
              "      <td>270.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.06</td>\n",
              "      <td>5.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>0.24980</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6240</td>\n",
              "      <td>5.857</td>\n",
              "      <td>98.2</td>\n",
              "      <td>1.6686</td>\n",
              "      <td>4.0</td>\n",
              "      <td>437.0</td>\n",
              "      <td>21.2</td>\n",
              "      <td>392.04</td>\n",
              "      <td>21.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>25.94060</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6790</td>\n",
              "      <td>5.304</td>\n",
              "      <td>89.1</td>\n",
              "      <td>1.6475</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>127.36</td>\n",
              "      <td>26.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>0.13587</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.59</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4890</td>\n",
              "      <td>6.064</td>\n",
              "      <td>59.1</td>\n",
              "      <td>4.2392</td>\n",
              "      <td>4.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>381.32</td>\n",
              "      <td>14.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>0.30347</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.38</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4930</td>\n",
              "      <td>6.312</td>\n",
              "      <td>28.9</td>\n",
              "      <td>5.4159</td>\n",
              "      <td>5.0</td>\n",
              "      <td>287.0</td>\n",
              "      <td>19.6</td>\n",
              "      <td>396.90</td>\n",
              "      <td>6.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>6.80117</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7130</td>\n",
              "      <td>6.081</td>\n",
              "      <td>84.4</td>\n",
              "      <td>2.7175</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>14.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>8.98296</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.7700</td>\n",
              "      <td>6.212</td>\n",
              "      <td>97.4</td>\n",
              "      <td>2.1222</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>377.73</td>\n",
              "      <td>17.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>45.74610</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6930</td>\n",
              "      <td>4.519</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.6582</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>88.27</td>\n",
              "      <td>36.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>10.67180</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.7400</td>\n",
              "      <td>6.459</td>\n",
              "      <td>94.8</td>\n",
              "      <td>1.9879</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>43.06</td>\n",
              "      <td>23.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>0.22969</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4890</td>\n",
              "      <td>6.326</td>\n",
              "      <td>52.5</td>\n",
              "      <td>4.3549</td>\n",
              "      <td>4.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>394.87</td>\n",
              "      <td>10.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>18.49820</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6680</td>\n",
              "      <td>4.138</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.1370</td>\n",
              "      <td>24.0</td>\n",
              "      <td>666.0</td>\n",
              "      <td>20.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>37.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>0.21977</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.91</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4480</td>\n",
              "      <td>5.602</td>\n",
              "      <td>62.0</td>\n",
              "      <td>6.0877</td>\n",
              "      <td>3.0</td>\n",
              "      <td>233.0</td>\n",
              "      <td>17.9</td>\n",
              "      <td>396.90</td>\n",
              "      <td>16.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>0.16211</td>\n",
              "      <td>20.0</td>\n",
              "      <td>6.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4640</td>\n",
              "      <td>6.240</td>\n",
              "      <td>16.3</td>\n",
              "      <td>4.4290</td>\n",
              "      <td>3.0</td>\n",
              "      <td>223.0</td>\n",
              "      <td>18.6</td>\n",
              "      <td>396.90</td>\n",
              "      <td>6.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>0.03466</td>\n",
              "      <td>35.0</td>\n",
              "      <td>6.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4379</td>\n",
              "      <td>6.031</td>\n",
              "      <td>23.3</td>\n",
              "      <td>6.6407</td>\n",
              "      <td>1.0</td>\n",
              "      <td>304.0</td>\n",
              "      <td>16.9</td>\n",
              "      <td>362.25</td>\n",
              "      <td>7.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>2.14918</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.58</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8710</td>\n",
              "      <td>5.709</td>\n",
              "      <td>98.5</td>\n",
              "      <td>1.6232</td>\n",
              "      <td>5.0</td>\n",
              "      <td>403.0</td>\n",
              "      <td>14.7</td>\n",
              "      <td>261.95</td>\n",
              "      <td>15.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>0.01439</td>\n",
              "      <td>60.0</td>\n",
              "      <td>2.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4010</td>\n",
              "      <td>6.604</td>\n",
              "      <td>18.8</td>\n",
              "      <td>6.2196</td>\n",
              "      <td>1.0</td>\n",
              "      <td>265.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>376.70</td>\n",
              "      <td>4.38</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>404 rows Ã— 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0     1      2    3       4      5      6       7     8      9   \\\n",
              "0     1.23247   0.0   8.14  0.0  0.5380  6.142   91.7  3.9769   4.0  307.0   \n",
              "1     0.02177  82.5   2.03  0.0  0.4150  7.610   15.7  6.2700   2.0  348.0   \n",
              "2     4.89822   0.0  18.10  0.0  0.6310  4.970  100.0  1.3325  24.0  666.0   \n",
              "3     0.03961   0.0   5.19  0.0  0.5150  6.037   34.5  5.9853   5.0  224.0   \n",
              "4     3.69311   0.0  18.10  0.0  0.7130  6.376   88.4  2.5671  24.0  666.0   \n",
              "5     0.28392   0.0   7.38  0.0  0.4930  5.708   74.3  4.7211   5.0  287.0   \n",
              "6     9.18702   0.0  18.10  0.0  0.7000  5.536  100.0  1.5804  24.0  666.0   \n",
              "7     4.09740   0.0  19.58  0.0  0.8710  5.468  100.0  1.4118   5.0  403.0   \n",
              "8     2.15505   0.0  19.58  0.0  0.8710  5.628  100.0  1.5166   5.0  403.0   \n",
              "9     1.62864   0.0  21.89  0.0  0.6240  5.019  100.0  1.4394   4.0  437.0   \n",
              "10    9.59571   0.0  18.10  0.0  0.6930  6.404  100.0  1.6390  24.0  666.0   \n",
              "11   18.81100   0.0  18.10  0.0  0.5970  4.628  100.0  1.5539  24.0  666.0   \n",
              "12    0.13914   0.0   4.05  0.0  0.5100  5.572   88.5  2.5961   5.0  296.0   \n",
              "13    3.83684   0.0  18.10  0.0  0.7700  6.251   91.1  2.2955  24.0  666.0   \n",
              "14    0.38735   0.0  25.65  0.0  0.5810  5.613   95.6  1.7572   2.0  188.0   \n",
              "15   73.53410   0.0  18.10  0.0  0.6790  5.957  100.0  1.8026  24.0  666.0   \n",
              "16    6.53876   0.0  18.10  1.0  0.6310  7.016   97.5  1.2024  24.0  666.0   \n",
              "17    0.06466  70.0   2.24  0.0  0.4000  6.345   20.1  7.8278   5.0  358.0   \n",
              "18    0.03445  82.5   2.03  0.0  0.4150  6.162   38.4  6.2700   2.0  348.0   \n",
              "19    0.14866   0.0   8.56  0.0  0.5200  6.727   79.9  2.7778   5.0  384.0   \n",
              "20   37.66190   0.0  18.10  0.0  0.6790  6.202   78.7  1.8629  24.0  666.0   \n",
              "21    0.02763  75.0   2.95  0.0  0.4280  6.595   21.8  5.4011   3.0  252.0   \n",
              "22    0.01778  95.0   1.47  0.0  0.4030  7.135   13.9  7.6534   3.0  402.0   \n",
              "23    0.00632  18.0   2.31  0.0  0.5380  6.575   65.2  4.0900   1.0  296.0   \n",
              "24    0.03041   0.0   5.19  0.0  0.5150  5.895   59.6  5.6150   5.0  224.0   \n",
              "25    9.82349   0.0  18.10  0.0  0.6710  6.794   98.8  1.3580  24.0  666.0   \n",
              "26    0.08829  12.5   7.87  0.0  0.5240  6.012   66.6  5.5605   5.0  311.0   \n",
              "27    0.02729   0.0   7.07  0.0  0.4690  7.185   61.1  4.9671   2.0  242.0   \n",
              "28    0.67191   0.0   8.14  0.0  0.5380  5.813   90.3  4.6820   4.0  307.0   \n",
              "29    0.17783   0.0   9.69  0.0  0.5850  5.569   73.5  2.3999   6.0  391.0   \n",
              "..        ...   ...    ...  ...     ...    ...    ...     ...   ...    ...   \n",
              "374   0.07013   0.0  13.89  0.0  0.5500  6.642   85.1  3.4211   5.0  276.0   \n",
              "375   0.05780   0.0   2.46  0.0  0.4880  6.980   58.4  2.8290   3.0  193.0   \n",
              "376   0.04684   0.0   3.41  0.0  0.4890  6.417   66.1  3.0923   2.0  270.0   \n",
              "377   4.81213   0.0  18.10  0.0  0.7130  6.701   90.0  2.5975  24.0  666.0   \n",
              "378   0.05360  21.0   5.64  0.0  0.4390  6.511   21.1  6.8147   4.0  243.0   \n",
              "379   0.12579  45.0   3.44  0.0  0.4370  6.556   29.1  4.5667   5.0  398.0   \n",
              "380   0.79041   0.0   9.90  0.0  0.5440  6.122   52.8  2.6403   4.0  304.0   \n",
              "381   0.32264   0.0  21.89  0.0  0.6240  5.942   93.5  1.9669   4.0  437.0   \n",
              "382   0.55778   0.0  21.89  0.0  0.6240  6.335   98.2  2.1107   4.0  437.0   \n",
              "383   0.04666  80.0   1.52  0.0  0.4040  7.107   36.6  7.3090   2.0  329.0   \n",
              "384   0.03113   0.0   4.39  0.0  0.4420  6.014   48.5  8.0136   3.0  352.0   \n",
              "385   0.17505   0.0   5.96  0.0  0.4990  5.966   30.2  3.8473   5.0  279.0   \n",
              "386   9.92485   0.0  18.10  0.0  0.7400  6.251   96.6  2.1980  24.0  666.0   \n",
              "387   0.11432   0.0   8.56  0.0  0.5200  6.781   71.3  2.8561   5.0  384.0   \n",
              "388   0.05302   0.0   3.41  0.0  0.4890  7.079   63.1  3.4145   2.0  270.0   \n",
              "389   0.24980   0.0  21.89  0.0  0.6240  5.857   98.2  1.6686   4.0  437.0   \n",
              "390  25.94060   0.0  18.10  0.0  0.6790  5.304   89.1  1.6475  24.0  666.0   \n",
              "391   0.13587   0.0  10.59  1.0  0.4890  6.064   59.1  4.2392   4.0  277.0   \n",
              "392   0.30347   0.0   7.38  0.0  0.4930  6.312   28.9  5.4159   5.0  287.0   \n",
              "393   6.80117   0.0  18.10  0.0  0.7130  6.081   84.4  2.7175  24.0  666.0   \n",
              "394   8.98296   0.0  18.10  1.0  0.7700  6.212   97.4  2.1222  24.0  666.0   \n",
              "395  45.74610   0.0  18.10  0.0  0.6930  4.519  100.0  1.6582  24.0  666.0   \n",
              "396  10.67180   0.0  18.10  0.0  0.7400  6.459   94.8  1.9879  24.0  666.0   \n",
              "397   0.22969   0.0  10.59  0.0  0.4890  6.326   52.5  4.3549   4.0  277.0   \n",
              "398  18.49820   0.0  18.10  0.0  0.6680  4.138  100.0  1.1370  24.0  666.0   \n",
              "399   0.21977   0.0   6.91  0.0  0.4480  5.602   62.0  6.0877   3.0  233.0   \n",
              "400   0.16211  20.0   6.96  0.0  0.4640  6.240   16.3  4.4290   3.0  223.0   \n",
              "401   0.03466  35.0   6.06  0.0  0.4379  6.031   23.3  6.6407   1.0  304.0   \n",
              "402   2.14918   0.0  19.58  0.0  0.8710  5.709   98.5  1.6232   5.0  403.0   \n",
              "403   0.01439  60.0   2.93  0.0  0.4010  6.604   18.8  6.2196   1.0  265.0   \n",
              "\n",
              "       10      11     12  \n",
              "0    21.0  396.90  18.72  \n",
              "1    14.7  395.38   3.11  \n",
              "2    20.2  375.52   3.26  \n",
              "3    20.2  396.90   8.01  \n",
              "4    20.2  391.43  14.65  \n",
              "5    19.6  391.13  11.74  \n",
              "6    20.2  396.90  23.60  \n",
              "7    14.7  396.90  26.42  \n",
              "8    14.7  169.27  16.65  \n",
              "9    21.2  396.90  34.41  \n",
              "10   20.2  376.11  20.31  \n",
              "11   20.2   28.79  34.37  \n",
              "12   16.6  396.90  14.69  \n",
              "13   20.2  350.65  14.19  \n",
              "14   19.1  359.29  27.26  \n",
              "15   20.2   16.45  20.62  \n",
              "16   20.2  392.05   2.96  \n",
              "17   14.8  368.24   4.97  \n",
              "18   14.7  393.77   7.43  \n",
              "19   20.9  394.76   9.42  \n",
              "20   20.2   18.82  14.52  \n",
              "21   18.3  395.63   4.32  \n",
              "22   17.0  384.30   4.45  \n",
              "23   15.3  396.90   4.98  \n",
              "24   20.2  394.81  10.56  \n",
              "25   20.2  396.90  21.24  \n",
              "26   15.2  395.60  12.43  \n",
              "27   17.8  392.83   4.03  \n",
              "28   21.0  376.88  14.81  \n",
              "29   19.2  395.77  15.10  \n",
              "..    ...     ...    ...  \n",
              "374  16.4  392.78   9.69  \n",
              "375  17.8  396.90   5.04  \n",
              "376  17.8  392.18   8.81  \n",
              "377  20.2  255.23  16.42  \n",
              "378  16.8  396.90   5.28  \n",
              "379  15.2  382.84   4.56  \n",
              "380  18.4  396.90   5.98  \n",
              "381  21.2  378.25  16.90  \n",
              "382  21.2  394.67  16.96  \n",
              "383  12.6  354.31   8.61  \n",
              "384  18.8  385.64  10.53  \n",
              "385  19.2  393.43  10.13  \n",
              "386  20.2  388.52  16.44  \n",
              "387  20.9  395.58   7.67  \n",
              "388  17.8  396.06   5.70  \n",
              "389  21.2  392.04  21.32  \n",
              "390  20.2  127.36  26.64  \n",
              "391  18.6  381.32  14.66  \n",
              "392  19.6  396.90   6.15  \n",
              "393  20.2  396.90  14.70  \n",
              "394  20.2  377.73  17.60  \n",
              "395  20.2   88.27  36.98  \n",
              "396  20.2   43.06  23.98  \n",
              "397  18.6  394.87  10.97  \n",
              "398  20.2  396.90  37.97  \n",
              "399  17.9  396.90  16.20  \n",
              "400  18.6  396.90   6.59  \n",
              "401  16.9  362.25   7.83  \n",
              "402  14.7  261.95  15.79  \n",
              "403  15.6  376.70   4.38  \n",
              "\n",
              "[404 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "fenzWyy5e0_S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example"
      ]
    },
    {
      "metadata": {
        "id": "OZQQc_zde05a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Processing**"
      ]
    },
    {
      "metadata": {
        "id": "x3aAs7t4eGrL",
        "colab_type": "code",
        "outputId": "062e4c85-7f4a-4784-9f98-3eb14917c014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "print(mean)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3.74511057e+00 1.14801980e+01 1.11044307e+01 6.18811881e-02\n",
            " 5.57355941e-01 6.26708168e+00 6.90106436e+01 3.74027079e+00\n",
            " 9.44059406e+00 4.05898515e+02 1.84759901e+01 3.54783168e+02\n",
            " 1.27408168e+01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_hV-7KFYft_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data -= mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BvxdoOQtf-FS",
        "colab_type": "code",
        "outputId": "39b24b6f-4ba4-4acb-d90e-a00fe71576ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "std = train_data.std(axis=0)\n",
        "print(std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.22929073e+00 2.37382770e+01 6.80287253e+00 2.40939633e-01\n",
            " 1.17147847e-01 7.08908627e-01 2.79060634e+01 2.02770050e+00\n",
            " 8.68758849e+00 1.66168506e+02 2.19765689e+00 9.39946015e+01\n",
            " 7.24556085e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wJex77ERgJWz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MTZD0drxgj2N",
        "colab_type": "code",
        "outputId": "905898ba-60c6-4db7-9694-71f72786e634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "1GJ62QFHllRW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Building \n"
      ]
    },
    {
      "metadata": {
        "id": "C5fSeDKqlqEC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(3, input_shape = (train_data.shape[1],), activation = 'relu'))\n",
        "  model.add(layers.Dense(3,  activation = 'relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss ='mse', metrics = ['mae'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCbozjoitJHl",
        "colab_type": "code",
        "outputId": "0c890a75-e645-423d-cd28-27077beef79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 3)                 42        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 4         \n",
            "=================================================================\n",
            "Total params: 58\n",
            "Trainable params: 58\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AzxAuw4pnMfs",
        "colab_type": "code",
        "outputId": "722cf729-ee7f-4ddd-c74f-f2c52d4bcb3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "k = 5 \n",
        "import numpy as np \n",
        "num_val_samples = len(train_data) // k \n",
        "num_epochs = 500 \n",
        "all_scores = [] \n",
        "for i in range(k): \n",
        "  print('processing fold #', i) \n",
        "  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] \n",
        "  val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] # Prepare the training data: data from all other partitions \n",
        "  partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) \n",
        "  partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0) # Build the Keras model (already compiled) \n",
        "  model = build_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n",
            "processing fold # 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "93wLtHIiuUhj",
        "colab_type": "code",
        "outputId": "acee1a29-0265-4f6b-899d-b32a34720032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17105
        }
      },
      "cell_type": "code",
      "source": [
        "all_mae_histories = []\n",
        "history = model.fit(partial_train_data, partial_train_targets,\n",
        "                   validation_data=(val_data, val_targets),\n",
        "                   epochs=num_epochs, batch_size=1, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 324 samples, validate on 80 samples\n",
            "Epoch 1/500\n",
            "324/324 [==============================] - 4s 14ms/step - loss: 553.3572 - mean_absolute_error: 21.6516 - val_loss: 638.3320 - val_mean_absolute_error: 23.2268\n",
            "Epoch 2/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 511.5075 - mean_absolute_error: 20.5520 - val_loss: 584.1575 - val_mean_absolute_error: 21.9050\n",
            "Epoch 3/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 440.0365 - mean_absolute_error: 18.5329 - val_loss: 491.0416 - val_mean_absolute_error: 19.5460\n",
            "Epoch 4/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 323.8731 - mean_absolute_error: 15.3026 - val_loss: 356.3776 - val_mean_absolute_error: 15.7812\n",
            "Epoch 5/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 200.9337 - mean_absolute_error: 11.3532 - val_loss: 229.1560 - val_mean_absolute_error: 11.5500\n",
            "Epoch 6/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 104.9847 - mean_absolute_error: 7.8894 - val_loss: 156.6528 - val_mean_absolute_error: 8.7171\n",
            "Epoch 7/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 71.0484 - mean_absolute_error: 6.1915 - val_loss: 134.2284 - val_mean_absolute_error: 7.6236\n",
            "Epoch 8/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 59.1164 - mean_absolute_error: 5.4345 - val_loss: 125.1969 - val_mean_absolute_error: 6.9565\n",
            "Epoch 9/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 52.5193 - mean_absolute_error: 4.9428 - val_loss: 118.4545 - val_mean_absolute_error: 6.5333\n",
            "Epoch 10/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 48.1624 - mean_absolute_error: 4.5891 - val_loss: 114.5510 - val_mean_absolute_error: 6.2746\n",
            "Epoch 11/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 45.1018 - mean_absolute_error: 4.3481 - val_loss: 112.0186 - val_mean_absolute_error: 6.0740\n",
            "Epoch 12/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 42.7489 - mean_absolute_error: 4.2039 - val_loss: 109.2841 - val_mean_absolute_error: 5.9167\n",
            "Epoch 13/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 41.0371 - mean_absolute_error: 4.0957 - val_loss: 106.8365 - val_mean_absolute_error: 5.7927\n",
            "Epoch 14/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 39.6002 - mean_absolute_error: 3.9969 - val_loss: 104.2005 - val_mean_absolute_error: 5.6857\n",
            "Epoch 15/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 38.3654 - mean_absolute_error: 3.9491 - val_loss: 102.7183 - val_mean_absolute_error: 5.6153\n",
            "Epoch 16/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 37.4951 - mean_absolute_error: 3.8678 - val_loss: 100.0747 - val_mean_absolute_error: 5.5705\n",
            "Epoch 17/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 36.4161 - mean_absolute_error: 3.8512 - val_loss: 98.2769 - val_mean_absolute_error: 5.4997\n",
            "Epoch 18/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 35.5156 - mean_absolute_error: 3.7891 - val_loss: 96.4623 - val_mean_absolute_error: 5.4265\n",
            "Epoch 19/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 34.4989 - mean_absolute_error: 3.7648 - val_loss: 95.1084 - val_mean_absolute_error: 5.3680\n",
            "Epoch 20/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 33.9505 - mean_absolute_error: 3.6874 - val_loss: 93.1075 - val_mean_absolute_error: 5.3399\n",
            "Epoch 21/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 33.1654 - mean_absolute_error: 3.6700 - val_loss: 91.4048 - val_mean_absolute_error: 5.2936\n",
            "Epoch 22/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 32.5277 - mean_absolute_error: 3.6456 - val_loss: 90.0488 - val_mean_absolute_error: 5.2317\n",
            "Epoch 23/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 31.9879 - mean_absolute_error: 3.5829 - val_loss: 88.1772 - val_mean_absolute_error: 5.1916\n",
            "Epoch 24/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 31.1722 - mean_absolute_error: 3.5792 - val_loss: 87.0594 - val_mean_absolute_error: 5.1379\n",
            "Epoch 25/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 30.5712 - mean_absolute_error: 3.5403 - val_loss: 85.4593 - val_mean_absolute_error: 5.0806\n",
            "Epoch 26/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 29.8771 - mean_absolute_error: 3.4797 - val_loss: 84.4618 - val_mean_absolute_error: 5.0320\n",
            "Epoch 27/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 29.5290 - mean_absolute_error: 3.4487 - val_loss: 82.1045 - val_mean_absolute_error: 4.9966\n",
            "Epoch 28/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 28.6655 - mean_absolute_error: 3.4397 - val_loss: 80.6618 - val_mean_absolute_error: 4.9333\n",
            "Epoch 29/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 28.2764 - mean_absolute_error: 3.3929 - val_loss: 79.2468 - val_mean_absolute_error: 4.8872\n",
            "Epoch 30/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 27.4968 - mean_absolute_error: 3.3685 - val_loss: 78.3265 - val_mean_absolute_error: 4.8494\n",
            "Epoch 31/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 27.4040 - mean_absolute_error: 3.3090 - val_loss: 76.3371 - val_mean_absolute_error: 4.8166\n",
            "Epoch 32/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 26.8115 - mean_absolute_error: 3.3203 - val_loss: 74.6584 - val_mean_absolute_error: 4.7497\n",
            "Epoch 33/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 26.1181 - mean_absolute_error: 3.2869 - val_loss: 73.1596 - val_mean_absolute_error: 4.7053\n",
            "Epoch 34/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 25.6306 - mean_absolute_error: 3.2528 - val_loss: 71.7073 - val_mean_absolute_error: 4.6566\n",
            "Epoch 35/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 25.2286 - mean_absolute_error: 3.2268 - val_loss: 69.9795 - val_mean_absolute_error: 4.6029\n",
            "Epoch 36/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 24.5473 - mean_absolute_error: 3.1947 - val_loss: 69.2699 - val_mean_absolute_error: 4.5632\n",
            "Epoch 37/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 24.2873 - mean_absolute_error: 3.1386 - val_loss: 67.5778 - val_mean_absolute_error: 4.5436\n",
            "Epoch 38/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 23.7174 - mean_absolute_error: 3.1469 - val_loss: 66.3126 - val_mean_absolute_error: 4.4887\n",
            "Epoch 39/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 23.4256 - mean_absolute_error: 3.0926 - val_loss: 65.3834 - val_mean_absolute_error: 4.4955\n",
            "Epoch 40/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 22.9847 - mean_absolute_error: 3.0994 - val_loss: 64.4560 - val_mean_absolute_error: 4.4103\n",
            "Epoch 41/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 22.6817 - mean_absolute_error: 3.0533 - val_loss: 63.2101 - val_mean_absolute_error: 4.3817\n",
            "Epoch 42/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 22.3247 - mean_absolute_error: 3.0256 - val_loss: 61.6251 - val_mean_absolute_error: 4.3217\n",
            "Epoch 43/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 22.0009 - mean_absolute_error: 3.0055 - val_loss: 59.9754 - val_mean_absolute_error: 4.2631\n",
            "Epoch 44/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 21.4510 - mean_absolute_error: 2.9771 - val_loss: 58.4632 - val_mean_absolute_error: 4.2053\n",
            "Epoch 45/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 20.9296 - mean_absolute_error: 2.9286 - val_loss: 56.8189 - val_mean_absolute_error: 4.1485\n",
            "Epoch 46/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 20.5100 - mean_absolute_error: 2.9020 - val_loss: 56.0009 - val_mean_absolute_error: 4.0940\n",
            "Epoch 47/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 20.5132 - mean_absolute_error: 2.8691 - val_loss: 54.8905 - val_mean_absolute_error: 4.0529\n",
            "Epoch 48/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 20.0216 - mean_absolute_error: 2.8575 - val_loss: 53.2687 - val_mean_absolute_error: 3.9960\n",
            "Epoch 49/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 19.9353 - mean_absolute_error: 2.8282 - val_loss: 52.6797 - val_mean_absolute_error: 3.9623\n",
            "Epoch 50/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 19.5623 - mean_absolute_error: 2.7953 - val_loss: 51.7747 - val_mean_absolute_error: 3.9231\n",
            "Epoch 51/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 19.1706 - mean_absolute_error: 2.7811 - val_loss: 50.3488 - val_mean_absolute_error: 3.8490\n",
            "Epoch 52/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 18.9850 - mean_absolute_error: 2.7351 - val_loss: 49.9935 - val_mean_absolute_error: 3.8201\n",
            "Epoch 53/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 18.6533 - mean_absolute_error: 2.7335 - val_loss: 49.5308 - val_mean_absolute_error: 3.7898\n",
            "Epoch 54/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 18.6221 - mean_absolute_error: 2.7436 - val_loss: 48.8647 - val_mean_absolute_error: 3.7387\n",
            "Epoch 55/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 18.4769 - mean_absolute_error: 2.7177 - val_loss: 48.4926 - val_mean_absolute_error: 3.7006\n",
            "Epoch 56/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 18.2945 - mean_absolute_error: 2.6866 - val_loss: 47.7085 - val_mean_absolute_error: 3.6670\n",
            "Epoch 57/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 18.0318 - mean_absolute_error: 2.6776 - val_loss: 47.7549 - val_mean_absolute_error: 3.6425\n",
            "Epoch 58/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 18.0300 - mean_absolute_error: 2.6568 - val_loss: 47.3885 - val_mean_absolute_error: 3.6536\n",
            "Epoch 59/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.7885 - mean_absolute_error: 2.6468 - val_loss: 46.9629 - val_mean_absolute_error: 3.6070\n",
            "Epoch 60/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.5337 - mean_absolute_error: 2.6327 - val_loss: 46.8098 - val_mean_absolute_error: 3.5565\n",
            "Epoch 61/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.4044 - mean_absolute_error: 2.6195 - val_loss: 46.6273 - val_mean_absolute_error: 3.5610\n",
            "Epoch 62/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.4714 - mean_absolute_error: 2.6058 - val_loss: 46.0132 - val_mean_absolute_error: 3.5170\n",
            "Epoch 63/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.2191 - mean_absolute_error: 2.6158 - val_loss: 45.4698 - val_mean_absolute_error: 3.4857\n",
            "Epoch 64/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.1299 - mean_absolute_error: 2.6112 - val_loss: 45.1087 - val_mean_absolute_error: 3.4501\n",
            "Epoch 65/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.0932 - mean_absolute_error: 2.5856 - val_loss: 44.8500 - val_mean_absolute_error: 3.4861\n",
            "Epoch 66/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 17.0050 - mean_absolute_error: 2.5810 - val_loss: 44.5076 - val_mean_absolute_error: 3.4133\n",
            "Epoch 67/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.8560 - mean_absolute_error: 2.5732 - val_loss: 44.8508 - val_mean_absolute_error: 3.3930\n",
            "Epoch 68/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.8984 - mean_absolute_error: 2.5622 - val_loss: 44.5617 - val_mean_absolute_error: 3.4486\n",
            "Epoch 69/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.8700 - mean_absolute_error: 2.5417 - val_loss: 43.9588 - val_mean_absolute_error: 3.4360\n",
            "Epoch 70/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.6861 - mean_absolute_error: 2.5505 - val_loss: 43.8834 - val_mean_absolute_error: 3.3860\n",
            "Epoch 71/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.5476 - mean_absolute_error: 2.5233 - val_loss: 43.9313 - val_mean_absolute_error: 3.4047\n",
            "Epoch 72/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.5789 - mean_absolute_error: 2.5452 - val_loss: 43.6343 - val_mean_absolute_error: 3.4332\n",
            "Epoch 73/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.3511 - mean_absolute_error: 2.5240 - val_loss: 43.0847 - val_mean_absolute_error: 3.3281\n",
            "Epoch 74/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.4318 - mean_absolute_error: 2.4927 - val_loss: 43.1066 - val_mean_absolute_error: 3.3728\n",
            "Epoch 75/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.4107 - mean_absolute_error: 2.5195 - val_loss: 43.2658 - val_mean_absolute_error: 3.3428\n",
            "Epoch 76/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.2410 - mean_absolute_error: 2.5088 - val_loss: 43.5458 - val_mean_absolute_error: 3.4540\n",
            "Epoch 77/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.1093 - mean_absolute_error: 2.5190 - val_loss: 43.1497 - val_mean_absolute_error: 3.3322\n",
            "Epoch 78/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.2030 - mean_absolute_error: 2.5016 - val_loss: 43.5703 - val_mean_absolute_error: 3.3834\n",
            "Epoch 79/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 16.1551 - mean_absolute_error: 2.4805 - val_loss: 43.2929 - val_mean_absolute_error: 3.4393\n",
            "Epoch 80/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.8522 - mean_absolute_error: 2.4847 - val_loss: 42.5400 - val_mean_absolute_error: 3.3276\n",
            "Epoch 81/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.9250 - mean_absolute_error: 2.5051 - val_loss: 42.0331 - val_mean_absolute_error: 3.3175\n",
            "Epoch 82/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.8108 - mean_absolute_error: 2.4956 - val_loss: 42.6309 - val_mean_absolute_error: 3.3759\n",
            "Epoch 83/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.7390 - mean_absolute_error: 2.4943 - val_loss: 42.1069 - val_mean_absolute_error: 3.3443\n",
            "Epoch 84/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.6425 - mean_absolute_error: 2.4920 - val_loss: 42.7067 - val_mean_absolute_error: 3.3734\n",
            "Epoch 85/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.7748 - mean_absolute_error: 2.4674 - val_loss: 41.8679 - val_mean_absolute_error: 3.2937\n",
            "Epoch 86/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.7334 - mean_absolute_error: 2.4713 - val_loss: 41.7957 - val_mean_absolute_error: 3.3077\n",
            "Epoch 87/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.6839 - mean_absolute_error: 2.4819 - val_loss: 41.9359 - val_mean_absolute_error: 3.2856\n",
            "Epoch 88/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.7087 - mean_absolute_error: 2.4822 - val_loss: 41.9608 - val_mean_absolute_error: 3.3317\n",
            "Epoch 89/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.5580 - mean_absolute_error: 2.4973 - val_loss: 41.6223 - val_mean_absolute_error: 3.3274\n",
            "Epoch 90/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.6614 - mean_absolute_error: 2.4821 - val_loss: 41.5027 - val_mean_absolute_error: 3.3180\n",
            "Epoch 91/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.4871 - mean_absolute_error: 2.4484 - val_loss: 41.4547 - val_mean_absolute_error: 3.3930\n",
            "Epoch 92/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.2932 - mean_absolute_error: 2.4561 - val_loss: 41.9350 - val_mean_absolute_error: 3.4329\n",
            "Epoch 93/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.3561 - mean_absolute_error: 2.4661 - val_loss: 41.4524 - val_mean_absolute_error: 3.3264\n",
            "Epoch 94/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.3736 - mean_absolute_error: 2.4548 - val_loss: 41.0542 - val_mean_absolute_error: 3.3280\n",
            "Epoch 95/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.1771 - mean_absolute_error: 2.4528 - val_loss: 40.6390 - val_mean_absolute_error: 3.2768\n",
            "Epoch 96/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.0154 - mean_absolute_error: 2.4334 - val_loss: 40.9094 - val_mean_absolute_error: 3.2718\n",
            "Epoch 97/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.0337 - mean_absolute_error: 2.4332 - val_loss: 40.5160 - val_mean_absolute_error: 3.2438\n",
            "Epoch 98/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.1501 - mean_absolute_error: 2.4571 - val_loss: 40.4143 - val_mean_absolute_error: 3.3179\n",
            "Epoch 99/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.0117 - mean_absolute_error: 2.4644 - val_loss: 40.2929 - val_mean_absolute_error: 3.2617\n",
            "Epoch 100/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 14.7710 - mean_absolute_error: 2.4353 - val_loss: 40.3999 - val_mean_absolute_error: 3.2676\n",
            "Epoch 101/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.8821 - mean_absolute_error: 2.4267 - val_loss: 40.5330 - val_mean_absolute_error: 3.2710\n",
            "Epoch 102/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 15.0516 - mean_absolute_error: 2.4328 - val_loss: 40.2045 - val_mean_absolute_error: 3.3182\n",
            "Epoch 103/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.8360 - mean_absolute_error: 2.4460 - val_loss: 40.2593 - val_mean_absolute_error: 3.3250\n",
            "Epoch 104/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.6799 - mean_absolute_error: 2.4455 - val_loss: 40.3231 - val_mean_absolute_error: 3.2795\n",
            "Epoch 105/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.8384 - mean_absolute_error: 2.4025 - val_loss: 40.5708 - val_mean_absolute_error: 3.3575\n",
            "Epoch 106/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.6815 - mean_absolute_error: 2.4177 - val_loss: 40.7972 - val_mean_absolute_error: 3.4423\n",
            "Epoch 107/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.7437 - mean_absolute_error: 2.4193 - val_loss: 40.1656 - val_mean_absolute_error: 3.3528\n",
            "Epoch 108/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.6476 - mean_absolute_error: 2.4132 - val_loss: 40.7476 - val_mean_absolute_error: 3.3936\n",
            "Epoch 109/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.6842 - mean_absolute_error: 2.4371 - val_loss: 39.9743 - val_mean_absolute_error: 3.3295\n",
            "Epoch 110/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.4148 - mean_absolute_error: 2.4379 - val_loss: 40.4152 - val_mean_absolute_error: 3.2765\n",
            "Epoch 111/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.5589 - mean_absolute_error: 2.4302 - val_loss: 40.2754 - val_mean_absolute_error: 3.2831\n",
            "Epoch 112/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.6973 - mean_absolute_error: 2.4247 - val_loss: 40.8047 - val_mean_absolute_error: 3.3687\n",
            "Epoch 113/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.6443 - mean_absolute_error: 2.4064 - val_loss: 40.6765 - val_mean_absolute_error: 3.4231\n",
            "Epoch 114/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.5300 - mean_absolute_error: 2.4083 - val_loss: 40.6954 - val_mean_absolute_error: 3.4339\n",
            "Epoch 115/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.4530 - mean_absolute_error: 2.4209 - val_loss: 40.3328 - val_mean_absolute_error: 3.2877\n",
            "Epoch 116/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.5223 - mean_absolute_error: 2.4207 - val_loss: 40.5460 - val_mean_absolute_error: 3.3498\n",
            "Epoch 117/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.5589 - mean_absolute_error: 2.4167 - val_loss: 40.4546 - val_mean_absolute_error: 3.3717\n",
            "Epoch 118/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.3119 - mean_absolute_error: 2.4163 - val_loss: 40.0539 - val_mean_absolute_error: 3.2890\n",
            "Epoch 119/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.4674 - mean_absolute_error: 2.4084 - val_loss: 39.7478 - val_mean_absolute_error: 3.3505\n",
            "Epoch 120/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.3469 - mean_absolute_error: 2.4066 - val_loss: 40.0903 - val_mean_absolute_error: 3.4265\n",
            "Epoch 121/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.3107 - mean_absolute_error: 2.4255 - val_loss: 39.5889 - val_mean_absolute_error: 3.2942\n",
            "Epoch 122/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.4736 - mean_absolute_error: 2.4082 - val_loss: 40.0636 - val_mean_absolute_error: 3.3907\n",
            "Epoch 123/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.1806 - mean_absolute_error: 2.4245 - val_loss: 40.5942 - val_mean_absolute_error: 3.3305\n",
            "Epoch 124/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.2846 - mean_absolute_error: 2.3980 - val_loss: 39.9393 - val_mean_absolute_error: 3.3632\n",
            "Epoch 125/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.2345 - mean_absolute_error: 2.4126 - val_loss: 40.1891 - val_mean_absolute_error: 3.3107\n",
            "Epoch 126/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.2833 - mean_absolute_error: 2.4016 - val_loss: 40.1839 - val_mean_absolute_error: 3.3961\n",
            "Epoch 127/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.2830 - mean_absolute_error: 2.4083 - val_loss: 39.6331 - val_mean_absolute_error: 3.3149\n",
            "Epoch 128/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.0848 - mean_absolute_error: 2.4241 - val_loss: 40.0974 - val_mean_absolute_error: 3.3100\n",
            "Epoch 129/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.1699 - mean_absolute_error: 2.3809 - val_loss: 39.9105 - val_mean_absolute_error: 3.3356\n",
            "Epoch 130/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.1588 - mean_absolute_error: 2.4072 - val_loss: 39.3671 - val_mean_absolute_error: 3.2901\n",
            "Epoch 131/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.0704 - mean_absolute_error: 2.3801 - val_loss: 39.2650 - val_mean_absolute_error: 3.3343\n",
            "Epoch 132/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.1362 - mean_absolute_error: 2.3991 - val_loss: 39.2878 - val_mean_absolute_error: 3.3077\n",
            "Epoch 133/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.1946 - mean_absolute_error: 2.4033 - val_loss: 39.4852 - val_mean_absolute_error: 3.3517\n",
            "Epoch 134/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 14.0000 - mean_absolute_error: 2.4027 - val_loss: 39.3189 - val_mean_absolute_error: 3.3511\n",
            "Epoch 135/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.8849 - mean_absolute_error: 2.3778 - val_loss: 39.6096 - val_mean_absolute_error: 3.2958\n",
            "Epoch 136/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.9729 - mean_absolute_error: 2.3765 - val_loss: 39.4748 - val_mean_absolute_error: 3.3883\n",
            "Epoch 137/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.8607 - mean_absolute_error: 2.4118 - val_loss: 39.7383 - val_mean_absolute_error: 3.3490\n",
            "Epoch 138/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.9547 - mean_absolute_error: 2.3909 - val_loss: 38.9883 - val_mean_absolute_error: 3.2729\n",
            "Epoch 139/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.9691 - mean_absolute_error: 2.4041 - val_loss: 38.7419 - val_mean_absolute_error: 3.3045\n",
            "Epoch 140/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.7805 - mean_absolute_error: 2.3880 - val_loss: 39.5502 - val_mean_absolute_error: 3.3299\n",
            "Epoch 141/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.8810 - mean_absolute_error: 2.3793 - val_loss: 39.4518 - val_mean_absolute_error: 3.3991\n",
            "Epoch 142/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.8665 - mean_absolute_error: 2.3894 - val_loss: 38.9355 - val_mean_absolute_error: 3.3144\n",
            "Epoch 143/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.7762 - mean_absolute_error: 2.3669 - val_loss: 39.0920 - val_mean_absolute_error: 3.3824\n",
            "Epoch 144/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.7984 - mean_absolute_error: 2.3765 - val_loss: 38.5586 - val_mean_absolute_error: 3.2929\n",
            "Epoch 145/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.6471 - mean_absolute_error: 2.3596 - val_loss: 39.8063 - val_mean_absolute_error: 3.4775\n",
            "Epoch 146/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.7423 - mean_absolute_error: 2.4097 - val_loss: 38.7401 - val_mean_absolute_error: 3.3421\n",
            "Epoch 147/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.7366 - mean_absolute_error: 2.3737 - val_loss: 38.4835 - val_mean_absolute_error: 3.2940\n",
            "Epoch 148/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.7802 - mean_absolute_error: 2.3493 - val_loss: 38.4913 - val_mean_absolute_error: 3.3642\n",
            "Epoch 149/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.5510 - mean_absolute_error: 2.3630 - val_loss: 38.6282 - val_mean_absolute_error: 3.2753\n",
            "Epoch 150/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.6941 - mean_absolute_error: 2.3571 - val_loss: 38.6678 - val_mean_absolute_error: 3.2929\n",
            "Epoch 151/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.6890 - mean_absolute_error: 2.3629 - val_loss: 38.5791 - val_mean_absolute_error: 3.3059\n",
            "Epoch 152/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.6010 - mean_absolute_error: 2.3700 - val_loss: 38.0793 - val_mean_absolute_error: 3.2545\n",
            "Epoch 153/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.6097 - mean_absolute_error: 2.3610 - val_loss: 38.6188 - val_mean_absolute_error: 3.3494\n",
            "Epoch 154/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4883 - mean_absolute_error: 2.3613 - val_loss: 37.9484 - val_mean_absolute_error: 3.2555\n",
            "Epoch 155/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 13.5468 - mean_absolute_error: 2.3544 - val_loss: 38.1802 - val_mean_absolute_error: 3.2996\n",
            "Epoch 156/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.6424 - mean_absolute_error: 2.3604 - val_loss: 38.2342 - val_mean_absolute_error: 3.2742\n",
            "Epoch 157/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4574 - mean_absolute_error: 2.3761 - val_loss: 38.2611 - val_mean_absolute_error: 3.2538\n",
            "Epoch 158/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 13.6225 - mean_absolute_error: 2.3401 - val_loss: 38.6779 - val_mean_absolute_error: 3.3620\n",
            "Epoch 159/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.5467 - mean_absolute_error: 2.3492 - val_loss: 38.9422 - val_mean_absolute_error: 3.4204\n",
            "Epoch 160/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4479 - mean_absolute_error: 2.3539 - val_loss: 37.9333 - val_mean_absolute_error: 3.2573\n",
            "Epoch 161/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4948 - mean_absolute_error: 2.3670 - val_loss: 38.2220 - val_mean_absolute_error: 3.3102\n",
            "Epoch 162/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 13.4276 - mean_absolute_error: 2.3443 - val_loss: 38.2292 - val_mean_absolute_error: 3.2885\n",
            "Epoch 163/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4563 - mean_absolute_error: 2.3669 - val_loss: 38.6390 - val_mean_absolute_error: 3.2721\n",
            "Epoch 164/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.5284 - mean_absolute_error: 2.3581 - val_loss: 38.5810 - val_mean_absolute_error: 3.2947\n",
            "Epoch 165/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4685 - mean_absolute_error: 2.3490 - val_loss: 37.8460 - val_mean_absolute_error: 3.2630\n",
            "Epoch 166/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4817 - mean_absolute_error: 2.3470 - val_loss: 38.3834 - val_mean_absolute_error: 3.2732\n",
            "Epoch 167/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4647 - mean_absolute_error: 2.3582 - val_loss: 37.5601 - val_mean_absolute_error: 3.2567\n",
            "Epoch 168/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4344 - mean_absolute_error: 2.3543 - val_loss: 37.6474 - val_mean_absolute_error: 3.2631\n",
            "Epoch 169/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.3975 - mean_absolute_error: 2.3324 - val_loss: 37.8013 - val_mean_absolute_error: 3.3378\n",
            "Epoch 170/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4027 - mean_absolute_error: 2.3639 - val_loss: 38.0955 - val_mean_absolute_error: 3.2910\n",
            "Epoch 171/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4414 - mean_absolute_error: 2.3611 - val_loss: 37.8081 - val_mean_absolute_error: 3.2515\n",
            "Epoch 172/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.4609 - mean_absolute_error: 2.3387 - val_loss: 37.1939 - val_mean_absolute_error: 3.2250\n",
            "Epoch 173/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.3363 - mean_absolute_error: 2.3476 - val_loss: 37.3481 - val_mean_absolute_error: 3.2450\n",
            "Epoch 174/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.3536 - mean_absolute_error: 2.3572 - val_loss: 37.4356 - val_mean_absolute_error: 3.2527\n",
            "Epoch 175/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.3227 - mean_absolute_error: 2.3575 - val_loss: 37.2192 - val_mean_absolute_error: 3.2905\n",
            "Epoch 176/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.3128 - mean_absolute_error: 2.3447 - val_loss: 37.5313 - val_mean_absolute_error: 3.2756\n",
            "Epoch 177/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1403 - mean_absolute_error: 2.3255 - val_loss: 37.4132 - val_mean_absolute_error: 3.3764\n",
            "Epoch 178/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.2091 - mean_absolute_error: 2.3573 - val_loss: 37.0684 - val_mean_absolute_error: 3.2436\n",
            "Epoch 179/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.2426 - mean_absolute_error: 2.3445 - val_loss: 37.4414 - val_mean_absolute_error: 3.3089\n",
            "Epoch 180/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.2332 - mean_absolute_error: 2.3403 - val_loss: 36.8588 - val_mean_absolute_error: 3.2787\n",
            "Epoch 181/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1548 - mean_absolute_error: 2.3593 - val_loss: 37.0862 - val_mean_absolute_error: 3.2486\n",
            "Epoch 182/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0979 - mean_absolute_error: 2.3469 - val_loss: 37.1514 - val_mean_absolute_error: 3.3158\n",
            "Epoch 183/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.2328 - mean_absolute_error: 2.3465 - val_loss: 37.6449 - val_mean_absolute_error: 3.3476\n",
            "Epoch 184/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0418 - mean_absolute_error: 2.3272 - val_loss: 36.5573 - val_mean_absolute_error: 3.2445\n",
            "Epoch 185/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0216 - mean_absolute_error: 2.3091 - val_loss: 37.8311 - val_mean_absolute_error: 3.4100\n",
            "Epoch 186/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1070 - mean_absolute_error: 2.3224 - val_loss: 36.8555 - val_mean_absolute_error: 3.2853\n",
            "Epoch 187/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1331 - mean_absolute_error: 2.3418 - val_loss: 37.3951 - val_mean_absolute_error: 3.3182\n",
            "Epoch 188/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1304 - mean_absolute_error: 2.3234 - val_loss: 37.5395 - val_mean_absolute_error: 3.4108\n",
            "Epoch 189/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0884 - mean_absolute_error: 2.3401 - val_loss: 36.9355 - val_mean_absolute_error: 3.3468\n",
            "Epoch 190/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0543 - mean_absolute_error: 2.3220 - val_loss: 37.5193 - val_mean_absolute_error: 3.3206\n",
            "Epoch 191/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1502 - mean_absolute_error: 2.3427 - val_loss: 36.9765 - val_mean_absolute_error: 3.2891\n",
            "Epoch 192/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0580 - mean_absolute_error: 2.3339 - val_loss: 37.6671 - val_mean_absolute_error: 3.2722\n",
            "Epoch 193/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.2773 - mean_absolute_error: 2.3352 - val_loss: 37.5398 - val_mean_absolute_error: 3.3222\n",
            "Epoch 194/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0435 - mean_absolute_error: 2.3228 - val_loss: 37.4499 - val_mean_absolute_error: 3.2898\n",
            "Epoch 195/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.2475 - mean_absolute_error: 2.3249 - val_loss: 37.5348 - val_mean_absolute_error: 3.3440\n",
            "Epoch 196/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0995 - mean_absolute_error: 2.3342 - val_loss: 37.9979 - val_mean_absolute_error: 3.3370\n",
            "Epoch 197/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 13.2506 - mean_absolute_error: 2.3395 - val_loss: 37.0139 - val_mean_absolute_error: 3.2798\n",
            "Epoch 198/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1393 - mean_absolute_error: 2.3510 - val_loss: 37.5674 - val_mean_absolute_error: 3.3098\n",
            "Epoch 199/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0726 - mean_absolute_error: 2.3365 - val_loss: 38.0870 - val_mean_absolute_error: 3.3754\n",
            "Epoch 200/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1640 - mean_absolute_error: 2.3360 - val_loss: 37.0803 - val_mean_absolute_error: 3.2887\n",
            "Epoch 201/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0659 - mean_absolute_error: 2.3198 - val_loss: 37.0429 - val_mean_absolute_error: 3.2674\n",
            "Epoch 202/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0693 - mean_absolute_error: 2.3389 - val_loss: 37.2149 - val_mean_absolute_error: 3.3399\n",
            "Epoch 203/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0490 - mean_absolute_error: 2.3462 - val_loss: 37.3391 - val_mean_absolute_error: 3.3417\n",
            "Epoch 204/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9228 - mean_absolute_error: 2.3001 - val_loss: 37.4244 - val_mean_absolute_error: 3.3096\n",
            "Epoch 205/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0447 - mean_absolute_error: 2.3383 - val_loss: 36.7095 - val_mean_absolute_error: 3.2927\n",
            "Epoch 206/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0532 - mean_absolute_error: 2.3128 - val_loss: 36.9301 - val_mean_absolute_error: 3.3177\n",
            "Epoch 207/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9389 - mean_absolute_error: 2.3245 - val_loss: 36.8798 - val_mean_absolute_error: 3.2871\n",
            "Epoch 208/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0338 - mean_absolute_error: 2.3294 - val_loss: 36.8604 - val_mean_absolute_error: 3.3781\n",
            "Epoch 209/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9371 - mean_absolute_error: 2.3123 - val_loss: 37.0932 - val_mean_absolute_error: 3.3241\n",
            "Epoch 210/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8852 - mean_absolute_error: 2.3204 - val_loss: 37.5193 - val_mean_absolute_error: 3.2866\n",
            "Epoch 211/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 13.1176 - mean_absolute_error: 2.3353 - val_loss: 37.0902 - val_mean_absolute_error: 3.2817\n",
            "Epoch 212/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9535 - mean_absolute_error: 2.3389 - val_loss: 37.3044 - val_mean_absolute_error: 3.3494\n",
            "Epoch 213/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.9470 - mean_absolute_error: 2.3243 - val_loss: 36.9077 - val_mean_absolute_error: 3.2755\n",
            "Epoch 214/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0350 - mean_absolute_error: 2.3079 - val_loss: 37.7476 - val_mean_absolute_error: 3.4280\n",
            "Epoch 215/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9575 - mean_absolute_error: 2.3380 - val_loss: 37.5244 - val_mean_absolute_error: 3.3960\n",
            "Epoch 216/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0012 - mean_absolute_error: 2.3234 - val_loss: 36.8332 - val_mean_absolute_error: 3.3086\n",
            "Epoch 217/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7915 - mean_absolute_error: 2.3468 - val_loss: 37.5973 - val_mean_absolute_error: 3.4288\n",
            "Epoch 218/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8105 - mean_absolute_error: 2.3165 - val_loss: 36.9818 - val_mean_absolute_error: 3.2764\n",
            "Epoch 219/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 13.0435 - mean_absolute_error: 2.3152 - val_loss: 37.5289 - val_mean_absolute_error: 3.3973\n",
            "Epoch 220/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9412 - mean_absolute_error: 2.3307 - val_loss: 37.0359 - val_mean_absolute_error: 3.3694\n",
            "Epoch 221/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9423 - mean_absolute_error: 2.3411 - val_loss: 37.3781 - val_mean_absolute_error: 3.3697\n",
            "Epoch 222/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7670 - mean_absolute_error: 2.3252 - val_loss: 37.4326 - val_mean_absolute_error: 3.3165\n",
            "Epoch 223/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9628 - mean_absolute_error: 2.3183 - val_loss: 37.2729 - val_mean_absolute_error: 3.2927\n",
            "Epoch 224/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9432 - mean_absolute_error: 2.3384 - val_loss: 37.5968 - val_mean_absolute_error: 3.3594\n",
            "Epoch 225/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9619 - mean_absolute_error: 2.3359 - val_loss: 37.4071 - val_mean_absolute_error: 3.3192\n",
            "Epoch 226/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.1050 - mean_absolute_error: 2.3307 - val_loss: 37.5111 - val_mean_absolute_error: 3.3351\n",
            "Epoch 227/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8227 - mean_absolute_error: 2.3182 - val_loss: 37.9103 - val_mean_absolute_error: 3.3434\n",
            "Epoch 228/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8611 - mean_absolute_error: 2.3289 - val_loss: 37.4094 - val_mean_absolute_error: 3.3532\n",
            "Epoch 229/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8868 - mean_absolute_error: 2.3077 - val_loss: 37.4036 - val_mean_absolute_error: 3.3456\n",
            "Epoch 230/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8779 - mean_absolute_error: 2.3372 - val_loss: 37.4284 - val_mean_absolute_error: 3.3353\n",
            "Epoch 231/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8476 - mean_absolute_error: 2.3255 - val_loss: 37.7849 - val_mean_absolute_error: 3.3738\n",
            "Epoch 232/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9593 - mean_absolute_error: 2.3142 - val_loss: 37.7708 - val_mean_absolute_error: 3.3496\n",
            "Epoch 233/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9662 - mean_absolute_error: 2.3187 - val_loss: 37.4852 - val_mean_absolute_error: 3.3117\n",
            "Epoch 234/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8063 - mean_absolute_error: 2.3311 - val_loss: 38.1119 - val_mean_absolute_error: 3.4095\n",
            "Epoch 235/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8677 - mean_absolute_error: 2.3204 - val_loss: 37.5102 - val_mean_absolute_error: 3.3771\n",
            "Epoch 236/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8374 - mean_absolute_error: 2.3230 - val_loss: 37.9385 - val_mean_absolute_error: 3.4085\n",
            "Epoch 237/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9229 - mean_absolute_error: 2.3429 - val_loss: 37.7832 - val_mean_absolute_error: 3.3679\n",
            "Epoch 238/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9291 - mean_absolute_error: 2.3170 - val_loss: 38.0429 - val_mean_absolute_error: 3.3915\n",
            "Epoch 239/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8254 - mean_absolute_error: 2.3415 - val_loss: 38.0407 - val_mean_absolute_error: 3.3621\n",
            "Epoch 240/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0761 - mean_absolute_error: 2.3328 - val_loss: 37.6912 - val_mean_absolute_error: 3.3544\n",
            "Epoch 241/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9362 - mean_absolute_error: 2.3184 - val_loss: 37.3714 - val_mean_absolute_error: 3.3470\n",
            "Epoch 242/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8732 - mean_absolute_error: 2.3265 - val_loss: 38.0680 - val_mean_absolute_error: 3.4366\n",
            "Epoch 243/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7550 - mean_absolute_error: 2.3132 - val_loss: 38.3110 - val_mean_absolute_error: 3.4395\n",
            "Epoch 244/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9301 - mean_absolute_error: 2.3319 - val_loss: 37.2108 - val_mean_absolute_error: 3.3392\n",
            "Epoch 245/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9378 - mean_absolute_error: 2.3366 - val_loss: 37.7718 - val_mean_absolute_error: 3.3601\n",
            "Epoch 246/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9564 - mean_absolute_error: 2.3331 - val_loss: 37.6662 - val_mean_absolute_error: 3.3521\n",
            "Epoch 247/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7618 - mean_absolute_error: 2.3367 - val_loss: 37.8849 - val_mean_absolute_error: 3.3336\n",
            "Epoch 248/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9965 - mean_absolute_error: 2.3126 - val_loss: 37.6621 - val_mean_absolute_error: 3.4185\n",
            "Epoch 249/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.5412 - mean_absolute_error: 2.3268 - val_loss: 38.2555 - val_mean_absolute_error: 3.3553\n",
            "Epoch 250/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 13.0234 - mean_absolute_error: 2.3233 - val_loss: 37.7457 - val_mean_absolute_error: 3.3757\n",
            "Epoch 251/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9037 - mean_absolute_error: 2.3336 - val_loss: 38.2604 - val_mean_absolute_error: 3.4112\n",
            "Epoch 252/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7663 - mean_absolute_error: 2.3240 - val_loss: 38.1818 - val_mean_absolute_error: 3.3311\n",
            "Epoch 253/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.9858 - mean_absolute_error: 2.3306 - val_loss: 38.6837 - val_mean_absolute_error: 3.4091\n",
            "Epoch 254/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9563 - mean_absolute_error: 2.3253 - val_loss: 38.4859 - val_mean_absolute_error: 3.3838\n",
            "Epoch 255/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8863 - mean_absolute_error: 2.3287 - val_loss: 38.1816 - val_mean_absolute_error: 3.3645\n",
            "Epoch 256/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8830 - mean_absolute_error: 2.3427 - val_loss: 37.9913 - val_mean_absolute_error: 3.3612\n",
            "Epoch 257/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7762 - mean_absolute_error: 2.3300 - val_loss: 38.1278 - val_mean_absolute_error: 3.3507\n",
            "Epoch 258/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9747 - mean_absolute_error: 2.3228 - val_loss: 38.3252 - val_mean_absolute_error: 3.3946\n",
            "Epoch 259/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9144 - mean_absolute_error: 2.3093 - val_loss: 37.9680 - val_mean_absolute_error: 3.3809\n",
            "Epoch 260/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8203 - mean_absolute_error: 2.3129 - val_loss: 37.9264 - val_mean_absolute_error: 3.3418\n",
            "Epoch 261/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7074 - mean_absolute_error: 2.3144 - val_loss: 38.4215 - val_mean_absolute_error: 3.3783\n",
            "Epoch 262/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8212 - mean_absolute_error: 2.3328 - val_loss: 38.5517 - val_mean_absolute_error: 3.4189\n",
            "Epoch 263/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8153 - mean_absolute_error: 2.3395 - val_loss: 38.9171 - val_mean_absolute_error: 3.3949\n",
            "Epoch 264/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8332 - mean_absolute_error: 2.3444 - val_loss: 38.4597 - val_mean_absolute_error: 3.4011\n",
            "Epoch 265/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8267 - mean_absolute_error: 2.3276 - val_loss: 38.8580 - val_mean_absolute_error: 3.5058\n",
            "Epoch 266/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6864 - mean_absolute_error: 2.3048 - val_loss: 38.4262 - val_mean_absolute_error: 3.4797\n",
            "Epoch 267/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6910 - mean_absolute_error: 2.3243 - val_loss: 38.5180 - val_mean_absolute_error: 3.3988\n",
            "Epoch 268/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7834 - mean_absolute_error: 2.2926 - val_loss: 38.1201 - val_mean_absolute_error: 3.3649\n",
            "Epoch 269/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7179 - mean_absolute_error: 2.3248 - val_loss: 38.8974 - val_mean_absolute_error: 3.3937\n",
            "Epoch 270/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7887 - mean_absolute_error: 2.3154 - val_loss: 38.4131 - val_mean_absolute_error: 3.3502\n",
            "Epoch 271/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7763 - mean_absolute_error: 2.3069 - val_loss: 39.2266 - val_mean_absolute_error: 3.4588\n",
            "Epoch 272/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7452 - mean_absolute_error: 2.3065 - val_loss: 38.9875 - val_mean_absolute_error: 3.3815\n",
            "Epoch 273/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8482 - mean_absolute_error: 2.3188 - val_loss: 39.3894 - val_mean_absolute_error: 3.4452\n",
            "Epoch 274/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7715 - mean_absolute_error: 2.3035 - val_loss: 39.8148 - val_mean_absolute_error: 3.4505\n",
            "Epoch 275/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6995 - mean_absolute_error: 2.3097 - val_loss: 39.5597 - val_mean_absolute_error: 3.4086\n",
            "Epoch 276/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6258 - mean_absolute_error: 2.3062 - val_loss: 39.6051 - val_mean_absolute_error: 3.4097\n",
            "Epoch 277/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7685 - mean_absolute_error: 2.3022 - val_loss: 39.7372 - val_mean_absolute_error: 3.4172\n",
            "Epoch 278/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8932 - mean_absolute_error: 2.3176 - val_loss: 39.5917 - val_mean_absolute_error: 3.4110\n",
            "Epoch 279/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7804 - mean_absolute_error: 2.3102 - val_loss: 40.1518 - val_mean_absolute_error: 3.4728\n",
            "Epoch 280/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8890 - mean_absolute_error: 2.3202 - val_loss: 40.4041 - val_mean_absolute_error: 3.4664\n",
            "Epoch 281/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7784 - mean_absolute_error: 2.3158 - val_loss: 40.3832 - val_mean_absolute_error: 3.4755\n",
            "Epoch 282/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8686 - mean_absolute_error: 2.3150 - val_loss: 40.0705 - val_mean_absolute_error: 3.5071\n",
            "Epoch 283/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7900 - mean_absolute_error: 2.3334 - val_loss: 39.4962 - val_mean_absolute_error: 3.3817\n",
            "Epoch 284/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7038 - mean_absolute_error: 2.3074 - val_loss: 40.1768 - val_mean_absolute_error: 3.5507\n",
            "Epoch 285/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8939 - mean_absolute_error: 2.3266 - val_loss: 39.7458 - val_mean_absolute_error: 3.4606\n",
            "Epoch 286/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6914 - mean_absolute_error: 2.3070 - val_loss: 40.1848 - val_mean_absolute_error: 3.5331\n",
            "Epoch 287/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8097 - mean_absolute_error: 2.2906 - val_loss: 39.8020 - val_mean_absolute_error: 3.4124\n",
            "Epoch 288/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7524 - mean_absolute_error: 2.2822 - val_loss: 39.9833 - val_mean_absolute_error: 3.4300\n",
            "Epoch 289/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8121 - mean_absolute_error: 2.3064 - val_loss: 39.8669 - val_mean_absolute_error: 3.4224\n",
            "Epoch 290/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8057 - mean_absolute_error: 2.3161 - val_loss: 40.2716 - val_mean_absolute_error: 3.4566\n",
            "Epoch 291/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6549 - mean_absolute_error: 2.2860 - val_loss: 41.2329 - val_mean_absolute_error: 3.5504\n",
            "Epoch 292/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8062 - mean_absolute_error: 2.3105 - val_loss: 39.6058 - val_mean_absolute_error: 3.4159\n",
            "Epoch 293/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8308 - mean_absolute_error: 2.3046 - val_loss: 39.6278 - val_mean_absolute_error: 3.4446\n",
            "Epoch 294/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7352 - mean_absolute_error: 2.3077 - val_loss: 39.8441 - val_mean_absolute_error: 3.4377\n",
            "Epoch 295/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8658 - mean_absolute_error: 2.3216 - val_loss: 40.4670 - val_mean_absolute_error: 3.4603\n",
            "Epoch 296/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8624 - mean_absolute_error: 2.3182 - val_loss: 39.8581 - val_mean_absolute_error: 3.4266\n",
            "Epoch 297/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7315 - mean_absolute_error: 2.2952 - val_loss: 40.1757 - val_mean_absolute_error: 3.4726\n",
            "Epoch 298/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6802 - mean_absolute_error: 2.3199 - val_loss: 40.1667 - val_mean_absolute_error: 3.4696\n",
            "Epoch 299/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7648 - mean_absolute_error: 2.3083 - val_loss: 39.8406 - val_mean_absolute_error: 3.4036\n",
            "Epoch 300/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7926 - mean_absolute_error: 2.3303 - val_loss: 40.1718 - val_mean_absolute_error: 3.4719\n",
            "Epoch 301/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7430 - mean_absolute_error: 2.3167 - val_loss: 40.3345 - val_mean_absolute_error: 3.4863\n",
            "Epoch 302/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7228 - mean_absolute_error: 2.3153 - val_loss: 40.0243 - val_mean_absolute_error: 3.4519\n",
            "Epoch 303/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7137 - mean_absolute_error: 2.3247 - val_loss: 40.1622 - val_mean_absolute_error: 3.4381\n",
            "Epoch 304/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7667 - mean_absolute_error: 2.3216 - val_loss: 40.0069 - val_mean_absolute_error: 3.4293\n",
            "Epoch 305/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7447 - mean_absolute_error: 2.3184 - val_loss: 40.6825 - val_mean_absolute_error: 3.4108\n",
            "Epoch 306/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7379 - mean_absolute_error: 2.3186 - val_loss: 40.8679 - val_mean_absolute_error: 3.4550\n",
            "Epoch 307/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7616 - mean_absolute_error: 2.3079 - val_loss: 40.7063 - val_mean_absolute_error: 3.4363\n",
            "Epoch 308/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6960 - mean_absolute_error: 2.2898 - val_loss: 40.7486 - val_mean_absolute_error: 3.5643\n",
            "Epoch 309/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8234 - mean_absolute_error: 2.3195 - val_loss: 40.8426 - val_mean_absolute_error: 3.4526\n",
            "Epoch 310/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6526 - mean_absolute_error: 2.3259 - val_loss: 40.9400 - val_mean_absolute_error: 3.4729\n",
            "Epoch 311/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7603 - mean_absolute_error: 2.3184 - val_loss: 40.6901 - val_mean_absolute_error: 3.4425\n",
            "Epoch 312/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7931 - mean_absolute_error: 2.3110 - val_loss: 41.0997 - val_mean_absolute_error: 3.5533\n",
            "Epoch 313/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8065 - mean_absolute_error: 2.3155 - val_loss: 40.3787 - val_mean_absolute_error: 3.4369\n",
            "Epoch 314/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6744 - mean_absolute_error: 2.3174 - val_loss: 41.0775 - val_mean_absolute_error: 3.4766\n",
            "Epoch 315/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7897 - mean_absolute_error: 2.3104 - val_loss: 40.3122 - val_mean_absolute_error: 3.4166\n",
            "Epoch 316/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9455 - mean_absolute_error: 2.2954 - val_loss: 40.6767 - val_mean_absolute_error: 3.4720\n",
            "Epoch 317/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.5746 - mean_absolute_error: 2.3038 - val_loss: 41.9578 - val_mean_absolute_error: 3.5111\n",
            "Epoch 318/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8344 - mean_absolute_error: 2.3323 - val_loss: 41.3348 - val_mean_absolute_error: 3.5036\n",
            "Epoch 319/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7837 - mean_absolute_error: 2.2867 - val_loss: 41.0901 - val_mean_absolute_error: 3.5592\n",
            "Epoch 320/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7660 - mean_absolute_error: 2.3037 - val_loss: 41.3293 - val_mean_absolute_error: 3.5221\n",
            "Epoch 321/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7517 - mean_absolute_error: 2.2997 - val_loss: 41.4573 - val_mean_absolute_error: 3.5414\n",
            "Epoch 322/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8014 - mean_absolute_error: 2.3119 - val_loss: 40.3697 - val_mean_absolute_error: 3.4377\n",
            "Epoch 323/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6870 - mean_absolute_error: 2.3025 - val_loss: 40.3259 - val_mean_absolute_error: 3.4878\n",
            "Epoch 324/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7515 - mean_absolute_error: 2.3062 - val_loss: 40.5164 - val_mean_absolute_error: 3.4535\n",
            "Epoch 325/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7435 - mean_absolute_error: 2.2992 - val_loss: 40.9257 - val_mean_absolute_error: 3.4579\n",
            "Epoch 326/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8948 - mean_absolute_error: 2.2989 - val_loss: 40.8187 - val_mean_absolute_error: 3.4885\n",
            "Epoch 327/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7804 - mean_absolute_error: 2.2918 - val_loss: 41.2839 - val_mean_absolute_error: 3.6033\n",
            "Epoch 328/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.5439 - mean_absolute_error: 2.2938 - val_loss: 41.5829 - val_mean_absolute_error: 3.6072\n",
            "Epoch 329/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6311 - mean_absolute_error: 2.2980 - val_loss: 40.6389 - val_mean_absolute_error: 3.4187\n",
            "Epoch 330/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8588 - mean_absolute_error: 2.3227 - val_loss: 41.3645 - val_mean_absolute_error: 3.4917\n",
            "Epoch 331/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8640 - mean_absolute_error: 2.2986 - val_loss: 41.7692 - val_mean_absolute_error: 3.5520\n",
            "Epoch 332/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8209 - mean_absolute_error: 2.2929 - val_loss: 41.2937 - val_mean_absolute_error: 3.4877\n",
            "Epoch 333/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7334 - mean_absolute_error: 2.3036 - val_loss: 41.6041 - val_mean_absolute_error: 3.5617\n",
            "Epoch 334/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7762 - mean_absolute_error: 2.3008 - val_loss: 41.0783 - val_mean_absolute_error: 3.4628\n",
            "Epoch 335/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.5697 - mean_absolute_error: 2.2983 - val_loss: 42.4056 - val_mean_absolute_error: 3.4981\n",
            "Epoch 336/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8062 - mean_absolute_error: 2.3115 - val_loss: 41.6175 - val_mean_absolute_error: 3.5127\n",
            "Epoch 337/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6623 - mean_absolute_error: 2.3158 - val_loss: 41.6373 - val_mean_absolute_error: 3.4908\n",
            "Epoch 338/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8685 - mean_absolute_error: 2.2908 - val_loss: 41.6025 - val_mean_absolute_error: 3.5608\n",
            "Epoch 339/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7652 - mean_absolute_error: 2.3270 - val_loss: 41.1055 - val_mean_absolute_error: 3.4675\n",
            "Epoch 340/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6542 - mean_absolute_error: 2.3046 - val_loss: 41.6335 - val_mean_absolute_error: 3.4827\n",
            "Epoch 341/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7549 - mean_absolute_error: 2.3036 - val_loss: 40.6387 - val_mean_absolute_error: 3.4410\n",
            "Epoch 342/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7255 - mean_absolute_error: 2.3020 - val_loss: 40.5735 - val_mean_absolute_error: 3.4394\n",
            "Epoch 343/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8062 - mean_absolute_error: 2.3174 - val_loss: 40.9652 - val_mean_absolute_error: 3.4765\n",
            "Epoch 344/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6278 - mean_absolute_error: 2.2881 - val_loss: 41.2789 - val_mean_absolute_error: 3.6383\n",
            "Epoch 345/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7273 - mean_absolute_error: 2.3372 - val_loss: 41.9167 - val_mean_absolute_error: 3.5392\n",
            "Epoch 346/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7246 - mean_absolute_error: 2.3484 - val_loss: 41.6447 - val_mean_absolute_error: 3.4860\n",
            "Epoch 347/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7198 - mean_absolute_error: 2.3111 - val_loss: 41.1295 - val_mean_absolute_error: 3.4730\n",
            "Epoch 348/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7201 - mean_absolute_error: 2.3220 - val_loss: 41.3139 - val_mean_absolute_error: 3.4723\n",
            "Epoch 349/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7801 - mean_absolute_error: 2.3115 - val_loss: 41.1990 - val_mean_absolute_error: 3.5014\n",
            "Epoch 350/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7210 - mean_absolute_error: 2.3229 - val_loss: 41.2654 - val_mean_absolute_error: 3.4439\n",
            "Epoch 351/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8204 - mean_absolute_error: 2.3355 - val_loss: 41.7334 - val_mean_absolute_error: 3.5105\n",
            "Epoch 352/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6263 - mean_absolute_error: 2.3274 - val_loss: 41.3009 - val_mean_absolute_error: 3.5064\n",
            "Epoch 353/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7226 - mean_absolute_error: 2.2960 - val_loss: 41.4534 - val_mean_absolute_error: 3.4850\n",
            "Epoch 354/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7127 - mean_absolute_error: 2.2963 - val_loss: 41.7320 - val_mean_absolute_error: 3.4997\n",
            "Epoch 355/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7259 - mean_absolute_error: 2.3310 - val_loss: 41.4205 - val_mean_absolute_error: 3.4878\n",
            "Epoch 356/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7025 - mean_absolute_error: 2.3207 - val_loss: 41.0222 - val_mean_absolute_error: 3.4747\n",
            "Epoch 357/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7547 - mean_absolute_error: 2.3161 - val_loss: 41.5028 - val_mean_absolute_error: 3.5701\n",
            "Epoch 358/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.5982 - mean_absolute_error: 2.2898 - val_loss: 41.2632 - val_mean_absolute_error: 3.4526\n",
            "Epoch 359/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6784 - mean_absolute_error: 2.2724 - val_loss: 43.1630 - val_mean_absolute_error: 3.7335\n",
            "Epoch 360/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7318 - mean_absolute_error: 2.3521 - val_loss: 41.8706 - val_mean_absolute_error: 3.4953\n",
            "Epoch 361/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7643 - mean_absolute_error: 2.2914 - val_loss: 41.9732 - val_mean_absolute_error: 3.5589\n",
            "Epoch 362/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6422 - mean_absolute_error: 2.3265 - val_loss: 41.4248 - val_mean_absolute_error: 3.4765\n",
            "Epoch 363/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6661 - mean_absolute_error: 2.3204 - val_loss: 41.7211 - val_mean_absolute_error: 3.4677\n",
            "Epoch 364/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7762 - mean_absolute_error: 2.3052 - val_loss: 41.9470 - val_mean_absolute_error: 3.4762\n",
            "Epoch 365/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9862 - mean_absolute_error: 2.3072 - val_loss: 41.8724 - val_mean_absolute_error: 3.5364\n",
            "Epoch 366/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7104 - mean_absolute_error: 2.3125 - val_loss: 42.6063 - val_mean_absolute_error: 3.5634\n",
            "Epoch 367/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7717 - mean_absolute_error: 2.2959 - val_loss: 41.8430 - val_mean_absolute_error: 3.5033\n",
            "Epoch 368/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6345 - mean_absolute_error: 2.3119 - val_loss: 42.5500 - val_mean_absolute_error: 3.5434\n",
            "Epoch 369/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6661 - mean_absolute_error: 2.3205 - val_loss: 42.0937 - val_mean_absolute_error: 3.4705\n",
            "Epoch 370/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7358 - mean_absolute_error: 2.3130 - val_loss: 42.7403 - val_mean_absolute_error: 3.5011\n",
            "Epoch 371/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7901 - mean_absolute_error: 2.2951 - val_loss: 41.8408 - val_mean_absolute_error: 3.4660\n",
            "Epoch 372/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6962 - mean_absolute_error: 2.3082 - val_loss: 42.3660 - val_mean_absolute_error: 3.5111\n",
            "Epoch 373/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7472 - mean_absolute_error: 2.2982 - val_loss: 42.3646 - val_mean_absolute_error: 3.5399\n",
            "Epoch 374/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7301 - mean_absolute_error: 2.3114 - val_loss: 42.7062 - val_mean_absolute_error: 3.5308\n",
            "Epoch 375/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.5904 - mean_absolute_error: 2.3107 - val_loss: 42.2599 - val_mean_absolute_error: 3.4839\n",
            "Epoch 376/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6687 - mean_absolute_error: 2.3287 - val_loss: 42.0848 - val_mean_absolute_error: 3.4897\n",
            "Epoch 377/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8274 - mean_absolute_error: 2.3127 - val_loss: 42.6698 - val_mean_absolute_error: 3.5926\n",
            "Epoch 378/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6552 - mean_absolute_error: 2.3239 - val_loss: 42.2616 - val_mean_absolute_error: 3.4964\n",
            "Epoch 379/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8283 - mean_absolute_error: 2.2918 - val_loss: 42.2881 - val_mean_absolute_error: 3.5302\n",
            "Epoch 380/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6100 - mean_absolute_error: 2.3074 - val_loss: 42.1435 - val_mean_absolute_error: 3.4776\n",
            "Epoch 381/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7017 - mean_absolute_error: 2.3096 - val_loss: 42.6743 - val_mean_absolute_error: 3.5389\n",
            "Epoch 382/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7375 - mean_absolute_error: 2.3155 - val_loss: 42.7792 - val_mean_absolute_error: 3.5295\n",
            "Epoch 383/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7345 - mean_absolute_error: 2.3083 - val_loss: 41.9373 - val_mean_absolute_error: 3.4658\n",
            "Epoch 384/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6474 - mean_absolute_error: 2.3101 - val_loss: 42.2215 - val_mean_absolute_error: 3.4680\n",
            "Epoch 385/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8265 - mean_absolute_error: 2.3025 - val_loss: 42.4192 - val_mean_absolute_error: 3.6104\n",
            "Epoch 386/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.5454 - mean_absolute_error: 2.3224 - val_loss: 42.6035 - val_mean_absolute_error: 3.5268\n",
            "Epoch 387/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8222 - mean_absolute_error: 2.3051 - val_loss: 42.0345 - val_mean_absolute_error: 3.5504\n",
            "Epoch 388/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8643 - mean_absolute_error: 2.3024 - val_loss: 41.6089 - val_mean_absolute_error: 3.5457\n",
            "Epoch 389/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6336 - mean_absolute_error: 2.3287 - val_loss: 42.5308 - val_mean_absolute_error: 3.5532\n",
            "Epoch 390/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6441 - mean_absolute_error: 2.3364 - val_loss: 42.6052 - val_mean_absolute_error: 3.5107\n",
            "Epoch 391/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8239 - mean_absolute_error: 2.3207 - val_loss: 42.8311 - val_mean_absolute_error: 3.5979\n",
            "Epoch 392/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8506 - mean_absolute_error: 2.3186 - val_loss: 42.5448 - val_mean_absolute_error: 3.5282\n",
            "Epoch 393/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8647 - mean_absolute_error: 2.2875 - val_loss: 42.0214 - val_mean_absolute_error: 3.5080\n",
            "Epoch 394/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7415 - mean_absolute_error: 2.3179 - val_loss: 42.5416 - val_mean_absolute_error: 3.5281\n",
            "Epoch 395/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6326 - mean_absolute_error: 2.3133 - val_loss: 41.9552 - val_mean_absolute_error: 3.4953\n",
            "Epoch 396/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7362 - mean_absolute_error: 2.3297 - val_loss: 41.8830 - val_mean_absolute_error: 3.4821\n",
            "Epoch 397/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.5722 - mean_absolute_error: 2.3258 - val_loss: 42.0157 - val_mean_absolute_error: 3.4829\n",
            "Epoch 398/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6535 - mean_absolute_error: 2.2950 - val_loss: 42.6033 - val_mean_absolute_error: 3.6970\n",
            "Epoch 399/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6140 - mean_absolute_error: 2.3404 - val_loss: 42.1732 - val_mean_absolute_error: 3.4658\n",
            "Epoch 400/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7747 - mean_absolute_error: 2.3365 - val_loss: 42.9447 - val_mean_absolute_error: 3.5171\n",
            "Epoch 401/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7692 - mean_absolute_error: 2.3089 - val_loss: 42.2431 - val_mean_absolute_error: 3.5031\n",
            "Epoch 402/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6992 - mean_absolute_error: 2.3228 - val_loss: 42.7182 - val_mean_absolute_error: 3.5084\n",
            "Epoch 403/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8548 - mean_absolute_error: 2.3232 - val_loss: 42.8535 - val_mean_absolute_error: 3.4827\n",
            "Epoch 404/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.9306 - mean_absolute_error: 2.3159 - val_loss: 42.5248 - val_mean_absolute_error: 3.4996\n",
            "Epoch 405/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7676 - mean_absolute_error: 2.3442 - val_loss: 42.8497 - val_mean_absolute_error: 3.4922\n",
            "Epoch 406/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9181 - mean_absolute_error: 2.3334 - val_loss: 42.6593 - val_mean_absolute_error: 3.5160\n",
            "Epoch 407/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7082 - mean_absolute_error: 2.3146 - val_loss: 42.8304 - val_mean_absolute_error: 3.4746\n",
            "Epoch 408/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8422 - mean_absolute_error: 2.3341 - val_loss: 42.5728 - val_mean_absolute_error: 3.5769\n",
            "Epoch 409/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6578 - mean_absolute_error: 2.3091 - val_loss: 42.9484 - val_mean_absolute_error: 3.4850\n",
            "Epoch 410/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7630 - mean_absolute_error: 2.3354 - val_loss: 43.8492 - val_mean_absolute_error: 3.5639\n",
            "Epoch 411/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8631 - mean_absolute_error: 2.3116 - val_loss: 43.1267 - val_mean_absolute_error: 3.5511\n",
            "Epoch 412/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6979 - mean_absolute_error: 2.3353 - val_loss: 43.1579 - val_mean_absolute_error: 3.5225\n",
            "Epoch 413/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7373 - mean_absolute_error: 2.3125 - val_loss: 42.7729 - val_mean_absolute_error: 3.4898\n",
            "Epoch 414/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.5740 - mean_absolute_error: 2.3166 - val_loss: 43.4295 - val_mean_absolute_error: 3.6132\n",
            "Epoch 415/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6568 - mean_absolute_error: 2.3187 - val_loss: 43.0818 - val_mean_absolute_error: 3.4954\n",
            "Epoch 416/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7955 - mean_absolute_error: 2.3291 - val_loss: 43.6704 - val_mean_absolute_error: 3.5440\n",
            "Epoch 417/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7794 - mean_absolute_error: 2.3264 - val_loss: 43.5784 - val_mean_absolute_error: 3.5141\n",
            "Epoch 418/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7877 - mean_absolute_error: 2.3291 - val_loss: 43.8070 - val_mean_absolute_error: 3.5465\n",
            "Epoch 419/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7994 - mean_absolute_error: 2.3228 - val_loss: 43.8265 - val_mean_absolute_error: 3.5572\n",
            "Epoch 420/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7146 - mean_absolute_error: 2.3158 - val_loss: 43.8446 - val_mean_absolute_error: 3.6362\n",
            "Epoch 421/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6986 - mean_absolute_error: 2.3130 - val_loss: 43.6214 - val_mean_absolute_error: 3.5899\n",
            "Epoch 422/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7079 - mean_absolute_error: 2.3240 - val_loss: 44.0209 - val_mean_absolute_error: 3.5895\n",
            "Epoch 423/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6916 - mean_absolute_error: 2.3384 - val_loss: 43.8088 - val_mean_absolute_error: 3.5744\n",
            "Epoch 424/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6832 - mean_absolute_error: 2.3347 - val_loss: 43.2875 - val_mean_absolute_error: 3.5292\n",
            "Epoch 425/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7643 - mean_absolute_error: 2.3331 - val_loss: 43.4952 - val_mean_absolute_error: 3.5221\n",
            "Epoch 426/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6919 - mean_absolute_error: 2.3124 - val_loss: 44.0420 - val_mean_absolute_error: 3.5346\n",
            "Epoch 427/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8182 - mean_absolute_error: 2.3078 - val_loss: 44.0216 - val_mean_absolute_error: 3.5915\n",
            "Epoch 428/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7888 - mean_absolute_error: 2.3195 - val_loss: 43.3545 - val_mean_absolute_error: 3.5167\n",
            "Epoch 429/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6985 - mean_absolute_error: 2.3409 - val_loss: 43.6478 - val_mean_absolute_error: 3.5244\n",
            "Epoch 430/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6745 - mean_absolute_error: 2.3126 - val_loss: 43.2148 - val_mean_absolute_error: 3.5249\n",
            "Epoch 431/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6924 - mean_absolute_error: 2.2985 - val_loss: 43.3093 - val_mean_absolute_error: 3.6162\n",
            "Epoch 432/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6713 - mean_absolute_error: 2.3484 - val_loss: 43.2463 - val_mean_absolute_error: 3.5632\n",
            "Epoch 433/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6323 - mean_absolute_error: 2.3281 - val_loss: 44.1404 - val_mean_absolute_error: 3.5298\n",
            "Epoch 434/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8703 - mean_absolute_error: 2.3054 - val_loss: 43.2306 - val_mean_absolute_error: 3.5090\n",
            "Epoch 435/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6832 - mean_absolute_error: 2.3094 - val_loss: 42.9460 - val_mean_absolute_error: 3.4907\n",
            "Epoch 436/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8707 - mean_absolute_error: 2.3305 - val_loss: 43.6300 - val_mean_absolute_error: 3.6415\n",
            "Epoch 437/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7665 - mean_absolute_error: 2.3451 - val_loss: 43.6146 - val_mean_absolute_error: 3.5600\n",
            "Epoch 438/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7693 - mean_absolute_error: 2.3327 - val_loss: 43.9057 - val_mean_absolute_error: 3.5255\n",
            "Epoch 439/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7978 - mean_absolute_error: 2.3264 - val_loss: 43.6207 - val_mean_absolute_error: 3.5485\n",
            "Epoch 440/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.5933 - mean_absolute_error: 2.3317 - val_loss: 44.2337 - val_mean_absolute_error: 3.5349\n",
            "Epoch 441/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9693 - mean_absolute_error: 2.3233 - val_loss: 43.5404 - val_mean_absolute_error: 3.5341\n",
            "Epoch 442/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7715 - mean_absolute_error: 2.3108 - val_loss: 42.9983 - val_mean_absolute_error: 3.5200\n",
            "Epoch 443/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6577 - mean_absolute_error: 2.3201 - val_loss: 42.8545 - val_mean_absolute_error: 3.5041\n",
            "Epoch 444/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7312 - mean_absolute_error: 2.3248 - val_loss: 42.6524 - val_mean_absolute_error: 3.5239\n",
            "Epoch 445/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7811 - mean_absolute_error: 2.3137 - val_loss: 43.0568 - val_mean_absolute_error: 3.5154\n",
            "Epoch 446/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7868 - mean_absolute_error: 2.3034 - val_loss: 43.6796 - val_mean_absolute_error: 3.6690\n",
            "Epoch 447/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8042 - mean_absolute_error: 2.3000 - val_loss: 43.7897 - val_mean_absolute_error: 3.5450\n",
            "Epoch 448/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7476 - mean_absolute_error: 2.3055 - val_loss: 44.7099 - val_mean_absolute_error: 3.7075\n",
            "Epoch 449/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8213 - mean_absolute_error: 2.3404 - val_loss: 43.5608 - val_mean_absolute_error: 3.5285\n",
            "Epoch 450/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6750 - mean_absolute_error: 2.2969 - val_loss: 43.0772 - val_mean_absolute_error: 3.4891\n",
            "Epoch 451/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7440 - mean_absolute_error: 2.2855 - val_loss: 43.3151 - val_mean_absolute_error: 3.5678\n",
            "Epoch 452/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6805 - mean_absolute_error: 2.3060 - val_loss: 43.6948 - val_mean_absolute_error: 3.5467\n",
            "Epoch 453/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8552 - mean_absolute_error: 2.3235 - val_loss: 43.1269 - val_mean_absolute_error: 3.5017\n",
            "Epoch 454/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7935 - mean_absolute_error: 2.3237 - val_loss: 42.7835 - val_mean_absolute_error: 3.4737\n",
            "Epoch 455/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.5743 - mean_absolute_error: 2.3009 - val_loss: 42.8140 - val_mean_absolute_error: 3.4757\n",
            "Epoch 456/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.9519 - mean_absolute_error: 2.2977 - val_loss: 42.9031 - val_mean_absolute_error: 3.5164\n",
            "Epoch 457/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8800 - mean_absolute_error: 2.3286 - val_loss: 42.8322 - val_mean_absolute_error: 3.5222\n",
            "Epoch 458/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8064 - mean_absolute_error: 2.3213 - val_loss: 43.1958 - val_mean_absolute_error: 3.5411\n",
            "Epoch 459/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6424 - mean_absolute_error: 2.3206 - val_loss: 43.3335 - val_mean_absolute_error: 3.4858\n",
            "Epoch 460/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8809 - mean_absolute_error: 2.3093 - val_loss: 43.1527 - val_mean_absolute_error: 3.4956\n",
            "Epoch 461/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8832 - mean_absolute_error: 2.3158 - val_loss: 42.4077 - val_mean_absolute_error: 3.4970\n",
            "Epoch 462/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6780 - mean_absolute_error: 2.3020 - val_loss: 43.6299 - val_mean_absolute_error: 3.6441\n",
            "Epoch 463/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7289 - mean_absolute_error: 2.3306 - val_loss: 42.8084 - val_mean_absolute_error: 3.5010\n",
            "Epoch 464/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6694 - mean_absolute_error: 2.3094 - val_loss: 43.2237 - val_mean_absolute_error: 3.5539\n",
            "Epoch 465/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8352 - mean_absolute_error: 2.3251 - val_loss: 43.0667 - val_mean_absolute_error: 3.5285\n",
            "Epoch 466/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7793 - mean_absolute_error: 2.3273 - val_loss: 43.2663 - val_mean_absolute_error: 3.5108\n",
            "Epoch 467/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7732 - mean_absolute_error: 2.3075 - val_loss: 42.8120 - val_mean_absolute_error: 3.4849\n",
            "Epoch 468/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7599 - mean_absolute_error: 2.3190 - val_loss: 42.2864 - val_mean_absolute_error: 3.4732\n",
            "Epoch 469/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7490 - mean_absolute_error: 2.3131 - val_loss: 42.9012 - val_mean_absolute_error: 3.5332\n",
            "Epoch 470/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6193 - mean_absolute_error: 2.3299 - val_loss: 43.5594 - val_mean_absolute_error: 3.5314\n",
            "Epoch 471/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8616 - mean_absolute_error: 2.2891 - val_loss: 43.4771 - val_mean_absolute_error: 3.5785\n",
            "Epoch 472/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7660 - mean_absolute_error: 2.3015 - val_loss: 43.1789 - val_mean_absolute_error: 3.4925\n",
            "Epoch 473/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8290 - mean_absolute_error: 2.3231 - val_loss: 42.7654 - val_mean_absolute_error: 3.4988\n",
            "Epoch 474/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6443 - mean_absolute_error: 2.3081 - val_loss: 43.4587 - val_mean_absolute_error: 3.5031\n",
            "Epoch 475/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8790 - mean_absolute_error: 2.3296 - val_loss: 43.1738 - val_mean_absolute_error: 3.5053\n",
            "Epoch 476/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7968 - mean_absolute_error: 2.3041 - val_loss: 42.7683 - val_mean_absolute_error: 3.4788\n",
            "Epoch 477/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.9277 - mean_absolute_error: 2.3216 - val_loss: 43.0361 - val_mean_absolute_error: 3.4923\n",
            "Epoch 478/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7367 - mean_absolute_error: 2.3253 - val_loss: 42.5863 - val_mean_absolute_error: 3.5190\n",
            "Epoch 479/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7974 - mean_absolute_error: 2.3173 - val_loss: 42.9339 - val_mean_absolute_error: 3.5183\n",
            "Epoch 480/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6648 - mean_absolute_error: 2.2996 - val_loss: 42.7520 - val_mean_absolute_error: 3.5114\n",
            "Epoch 481/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6472 - mean_absolute_error: 2.3111 - val_loss: 43.7442 - val_mean_absolute_error: 3.7084\n",
            "Epoch 482/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6406 - mean_absolute_error: 2.3180 - val_loss: 42.5878 - val_mean_absolute_error: 3.4797\n",
            "Epoch 483/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.8233 - mean_absolute_error: 2.3092 - val_loss: 43.4140 - val_mean_absolute_error: 3.6507\n",
            "Epoch 484/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7490 - mean_absolute_error: 2.3238 - val_loss: 43.5778 - val_mean_absolute_error: 3.6227\n",
            "Epoch 485/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7569 - mean_absolute_error: 2.3347 - val_loss: 43.2824 - val_mean_absolute_error: 3.6062\n",
            "Epoch 486/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6776 - mean_absolute_error: 2.3282 - val_loss: 42.8043 - val_mean_absolute_error: 3.4968\n",
            "Epoch 487/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7121 - mean_absolute_error: 2.3300 - val_loss: 42.7426 - val_mean_absolute_error: 3.4877\n",
            "Epoch 488/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7569 - mean_absolute_error: 2.3109 - val_loss: 43.0477 - val_mean_absolute_error: 3.6040\n",
            "Epoch 489/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6247 - mean_absolute_error: 2.3070 - val_loss: 43.2787 - val_mean_absolute_error: 3.5911\n",
            "Epoch 490/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7280 - mean_absolute_error: 2.3281 - val_loss: 43.0906 - val_mean_absolute_error: 3.5219\n",
            "Epoch 491/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.8046 - mean_absolute_error: 2.2885 - val_loss: 42.8042 - val_mean_absolute_error: 3.5142\n",
            "Epoch 492/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7922 - mean_absolute_error: 2.3211 - val_loss: 42.6186 - val_mean_absolute_error: 3.4532\n",
            "Epoch 493/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6896 - mean_absolute_error: 2.3314 - val_loss: 42.9610 - val_mean_absolute_error: 3.4833\n",
            "Epoch 494/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7381 - mean_absolute_error: 2.3178 - val_loss: 43.0591 - val_mean_absolute_error: 3.5162\n",
            "Epoch 495/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.6376 - mean_absolute_error: 2.3316 - val_loss: 42.9577 - val_mean_absolute_error: 3.4963\n",
            "Epoch 496/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7720 - mean_absolute_error: 2.3023 - val_loss: 43.0869 - val_mean_absolute_error: 3.5616\n",
            "Epoch 497/500\n",
            "324/324 [==============================] - 1s 3ms/step - loss: 12.7583 - mean_absolute_error: 2.3367 - val_loss: 43.1483 - val_mean_absolute_error: 3.5343\n",
            "Epoch 498/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7818 - mean_absolute_error: 2.3009 - val_loss: 43.0260 - val_mean_absolute_error: 3.5647\n",
            "Epoch 499/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.7583 - mean_absolute_error: 2.3257 - val_loss: 43.0772 - val_mean_absolute_error: 3.5480\n",
            "Epoch 500/500\n",
            "324/324 [==============================] - 1s 4ms/step - loss: 12.6170 - mean_absolute_error: 2.3028 - val_loss: 42.2605 - val_mean_absolute_error: 3.5060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NP1Lrr4Hv_3p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mae_history = history.history['val_mean_absolute_error']\n",
        "all_mae_histories.append(mae_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Dbn0hlazlTg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "average_mae_history = [\n",
        "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HtCjLlCszSV2",
        "colab_type": "code",
        "outputId": "367ea94d-5171-4666-a1c0-6c6a9ef2d80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "40R3dcixzar7",
        "colab_type": "code",
        "outputId": "8c669d72-1439-4e46-dac4-358e34151c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "cell_type": "code",
      "source": [
        "def smooth_curve(points, factor=0.9):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
        "\n",
        "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "443PMnITztWM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}